{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Approach to Non-SQL PTM Queries\n",
    "--\n",
    "\n",
    "Animate the overall field\n",
    "\n",
    "First go with lightly smoothed, subtidal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postproc_dask as post\n",
    "import six\n",
    "six.moves.reload_module(post)\n",
    "post.config_malloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.close()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing.popen_spawn_posix #  https://github.com/dask/distributed/issues/4168\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d690a7745ed4fca903287d6dc58f574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client(n_workers=20,threads_per_worker=1)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import conc_figure\n",
    "import six\n",
    "import stompy.plot.cmap as scmap\n",
    "from stompy.spatial import proj_utils\n",
    "from matplotlib import cm\n",
    "cmap=cm.CMRmap_r\n",
    "cmap=scmap.cmap_clip(cmap,0.03,1.0)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import re\n",
    "\n",
    "from stompy.grid import unstructured_grid\n",
    "from stompy import utils, memoize\n",
    "from stompy.model.fish_ptm import ptm_config, ptm_tools\n",
    "from stompy.model.suntans import sun_driver\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import stompy.plot.cmap as scmap\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "turbo=scmap.load_gradient('turbo.cpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Process\n",
    "===\n",
    "\n",
    "1. SUNTANS hydro runs\n",
    "2. SUNTANS average output\n",
    "3. ptm-formatted average output\n",
    "4. PTM runs\n",
    "5. Load data\n",
    "\n",
    "The top-level query is something like *generate a map of concentrations for...*\n",
    "\n",
    "filter on:\n",
    " - sources $x$\n",
    " - settling classes $y$\n",
    " - vertical positions $z$\n",
    " - horizontal positions $h$\n",
    "\n",
    "weighted by\n",
    "\n",
    " - loading data \n",
    " - age\n",
    " \n",
    "mapped by one of ...\n",
    "\n",
    " - bounding box\n",
    " - put on hydro grid\n",
    " - put on regular grid\n",
    "\n",
    "and possibly smoothed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment level configuration -- small, all python native data.\n",
    "# The 'new' run\n",
    "cfg=dict(\n",
    "    ptm_base_dir=\"/opt2/sfb_ocean/ptm/all_source_022b\",\n",
    "    sun_base_dir=\"/opt2/sfb_ocean/suntans/runs\",\n",
    "    ptm_output_interval=np.timedelta64(1,'h')\n",
    ")\n",
    "cfg['ptm_run_patt']=os.path.join(cfg['ptm_base_dir'],\"chunk??\",\"20??????\")\n",
    "cfg['sun_patt']=os.path.join(cfg['sun_base_dir'],\"merged_022_20??????\")\n",
    "\n",
    "ptm_run_paths=glob.glob(cfg['ptm_run_patt'])\n",
    "ptm_run_paths.sort()\n",
    "cfg['ptm_run_paths']=ptm_run_paths\n",
    "\n",
    "sun_paths=glob.glob(cfg['sun_patt'])\n",
    "sun_paths.sort()\n",
    "cfg['sun_paths']=sun_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grid into... grid\n",
    "hydro_path=sun_paths[0]\n",
    "ptm_ds=xr.open_dataset(os.path.join(hydro_path,\"ptm_average.nc_0000.nc\"))\n",
    "grid=unstructured_grid.UnstructuredGrid.read_ugrid(ptm_ds,dialect='fishptm')\n",
    "ptm_ds.close()   \n",
    "\n",
    "# distribute to workers ahead of time.\n",
    "grid_d=client.scatter(grid)\n",
    "cfg['grid_d']=grid_d # too far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:63004/99089\n"
     ]
    }
   ],
   "source": [
    "# So far this is only used locally.  Slow to compute (15s)\n",
    "Msmooth=grid.smooth_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'postproc_dask' from '/home/rusty/src/microplastic_sfbay/postprocess/postproc_dask.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six.moves.reload_module(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or could make this delayed and have it execute on each client?\n",
    "load_data_d=client.scatter(post.get_load_data())\n",
    "cfg['load_data_d']=load_data_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_area(g):\n",
    "    return g.cells_area().sum()\n",
    "\n",
    "total_area_d=dask.delayed(total_area)(grid_d)\n",
    "cfg['total_area_d']=total_area_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydro timestamps\n",
    "cfg['hydro_timestamps']=post.load_hydro_timestamps(sun_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_ds_d=client.scatter(post.bc_ds(cfg=cfg))\n",
    "cfg['bc_ds_d']=bc_ds_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas=grid.cells_area()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps\n",
    "--\n",
    "\n",
    "\n",
    "1. Recreate some of the figures from before, including on-grid smoothing.  \n",
    "  a. Sample plot: from the powerpoint. 2017-08-30 to 2017-09-14. Surface particles\n",
    "     max age of 10 days.   *This plot is similarish -- not exactly the same but close\n",
    "     enough to rule out fundamental errors*\n",
    "2. Pull out manta samples as before. Maybe skip putting it on the grid, just\n",
    "   query a radius.\n",
    "  a. Implement in this notebook\n",
    "  b. Move all of this to a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution around 21Aug2017\n",
    "# Could specify additional criteria here\n",
    "rec_DATE=\"2017-08-21\"\n",
    "\n",
    "df=post.particles_for_date(rec_DATE,cfg=cfg,cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_dir='surface-frames-v00'\n",
    "if not os.path.exists(frame_dir):\n",
    "    os.makedirs(frame_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days=np.arange(np.datetime64(\"2017-08-01\"),\n",
    "               np.datetime64(\"2018-06-30\"),\n",
    "               np.timedelta64(1,'D'))\n",
    "\n",
    "days_str=[str(d) for d in days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(1,2,figsize=(9,6))\n",
    "\n",
    "storm_factor=0.02\n",
    "tau_d=30\n",
    "\n",
    "for date in days:\n",
    "    dt=utils.to_datetime(date)\n",
    "    date_s=dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    conc_fn=os.path.join(frame_dir,\n",
    "                         f\"conc-storm{storm_factor}-tau{tau_d}-{dt.strftime('%Y%m%dT%H%M')}.nc\")\n",
    "    fig_fn=conc_fn.replace('.nc','.png')\n",
    "    \n",
    "    if os.path.exists(conc_fn):\n",
    "        ds_conc=xr.open_dataset(conc_fn)\n",
    "    else:\n",
    "        df=post.particles_for_date(date_s,cfg=cfg,cache=False)\n",
    "        \n",
    "        age=df['time'] - df['rel_time']\n",
    "        tau=np.timedelta64(tau_d,'D')\n",
    "        decay=np.exp( -age/tau ) \n",
    "\n",
    "        group_weight=post.group_weights(df,storm_factor)\n",
    "        \n",
    "        df['count']=group_weight*df['weight_time'] * decay * df['mp_per_particle']\n",
    "        \n",
    "        ds_conc=post.particles_to_conc(df,grid,'count')\n",
    "        ds_conc.to_netcdf(conc_fn)\n",
    "\n",
    "    ds_smooth=ds_conc.copy()\n",
    "    # 2 iterations seems good for removing particle noise\n",
    "    for _ in range(2):\n",
    "        ds_smooth['conc']=('cell',), Msmooth.dot(ds_smooth.conc.values)\n",
    "\n",
    "    fig.clf()\n",
    "    axs=[fig.add_subplot(1,2,1),\n",
    "         fig.add_subplot(1,2,2)]\n",
    "    \n",
    "    fig.subplots_adjust(left=0,right=1,top=1,bottom=0,wspace=0)\n",
    "\n",
    "    cf1=conc_figure.BayConcFigure(ds_smooth,grid=grid,fig=fig,ax=axs[1],\n",
    "                                  cax_loc=[0.82,0.37,0.02,0.45])\n",
    "    cf2=conc_figure.CoastalConcFigure(ds_smooth,grid=grid,fig=fig,ax=axs[0],\n",
    "                                     cax_loc=None)\n",
    "    for ax in axs:\n",
    "        ax.texts=[]\n",
    "\n",
    "    axs[0].text(0.5,0.9,date_s,transform=axs[0].transAxes,fontsize=15)\n",
    "    axs[0].axis([441742., 591985., 4076042., 4276366.])\n",
    "    axs[1].axis([522130., 582108., 4146136, 4226106.])\n",
    "    plt.draw()\n",
    "    fig.savefig(fig_fn,dpi=200)\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2017-08-01T00:00 - 2017-08-02T00:00\n",
      "Will repartition with 42 partitions\n"
     ]
    }
   ],
   "source": [
    "out_dir=\"hourly-out-v00\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "storm_factor=0.02\n",
    "tau_d=30\n",
    "\n",
    "# The hour/minutes are important to cast correctly.\n",
    "intervals=np.arange(np.datetime64(\"2017-08-01T00:00\"),\n",
    "                    np.datetime64(\"2018-06-30T00:00\"),\n",
    "                    np.timedelta64(1,'D'))\n",
    "\n",
    "for t_min,t_max in zip(intervals[:-1],intervals[1:]):\n",
    "    hours=np.arange(t_min,t_max,np.timedelta64(3600,'s'))\n",
    "    targets=[]\n",
    "    for h_min,h_max in zip(hours[:-1],hours[1:]):\n",
    "        dt=utils.to_datetime(h_min)\n",
    "        date_s=dt.strftime(\"%Y%m%dT%H%M\")\n",
    "        conc_fn=os.path.join(out_dir,f\"surface-{date_s}.nc\")\n",
    "        if not os.path.exists(conc_fn):\n",
    "            targets.append([h_min,h_max,conc_fn])\n",
    "    if not targets:\n",
    "        print(f\"Skip {t_min} - {t_max}\")\n",
    "        continue # nothing to do\n",
    "        \n",
    "    print(f\"Processing {t_min} - {t_max}\")\n",
    "        \n",
    "    criteria=dict(t_min=t_min,t_max=t_max,\n",
    "                  category='nonfiber',\n",
    "                  z_below_surface=0.095,\n",
    "                  age_max=np.timedelta64(60,'D'))\n",
    "    part_d=post.query_particles(criteria,cfg=cfg)\n",
    "    df=part_d.compute()\n",
    "    \n",
    "\n",
    "    # Weights computed on the whole group.\n",
    "    age=df['time'] - df['rel_time']\n",
    "    tau=np.timedelta64(tau_d,'D')\n",
    "    decay=np.exp( -age/tau ) \n",
    "\n",
    "    group_weight=post.group_weights(df,storm_factor)\n",
    "\n",
    "    df['count']=group_weight* decay * df['mp_per_particle']\n",
    "\n",
    "    for h_min,h_max,conc_fn in targets:\n",
    "        sel=(df['time']>=h_min).values & (df['time']<h_max).values\n",
    "        df_sel=df[sel]\n",
    "        ds_conc=post.particles_to_conc(df_sel,grid,'count')\n",
    "        ds_conc['time']=(),h_min\n",
    "        ds_conc['time_max']=(),h_max\n",
    "        ds_conc['storm_factor']=(),storm_factor\n",
    "        ds_conc['tau_d']=(),tau_d\n",
    "        ds_conc.to_netcdf(conc_fn)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
