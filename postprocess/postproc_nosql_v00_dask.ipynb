{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Approach\n",
    "\n",
    "Runs are bags\n",
    "\n",
    "query_particles:\n",
    " - Somewhere in here I want to switch to dask dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.067 GB maxrss\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "\n",
    "def print_rusage():\n",
    "    ru=resource.getrusage(resource.RUSAGE_SELF)\n",
    "    print(f\"{ru.ru_maxrss/1000/1000.:.3f} GB maxrss\")\n",
    "    # That's saying that this process is using 55GB of RAM!?\n",
    "    \n",
    "print_rusage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "client=Client(n_workers=4,threads_per_worker=1)\n",
    "# Might be problematic to do this later.  Not sure why I got an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.116 GB maxrss\n"
     ]
    }
   ],
   "source": [
    "print_rusage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0422589334434e76917fdc203de6b331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a more targeted postprocessing\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import re\n",
    "\n",
    "from stompy.grid import unstructured_grid\n",
    "from stompy import utils, memoize\n",
    "from stompy.model.fish_ptm import ptm_config, ptm_tools\n",
    "from stompy.model.suntans import sun_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stompy.plot.cmap as scmap\n",
    "turbo=scmap.load_gradient('turbo.cpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stompy.grid.unstructured_grid' from '/home/rusty/src/stompy/stompy/grid/unstructured_grid.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import six\n",
    "six.moves.reload_module(unstructured_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.222 GB maxrss\n"
     ]
    }
   ],
   "source": [
    "print_rusage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Process\n",
    "===\n",
    "\n",
    "1. SUNTANS hydro runs\n",
    "2. SUNTANS average output\n",
    "3. ptm-formatted average output\n",
    "4. PTM runs\n",
    "5. Load data\n",
    "\n",
    "The top-level query is something like *generate a map of concentrations for...*\n",
    "\n",
    "filter on:\n",
    " - sources $x$\n",
    " - settling classes $y$\n",
    " - vertical positions $z$\n",
    " - horizontal positions $h$\n",
    "\n",
    "weighted by\n",
    "\n",
    " - loading data \n",
    " - age\n",
    " \n",
    "mapped by one of ...\n",
    "\n",
    " - bounding box\n",
    " - put on hydro grid\n",
    " - put on regular grid\n",
    "\n",
    "and possibly smoothed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    ptm_base_dir=\"/opt2/sfb_ocean/ptm/all_source_022a\"\n",
    "    sun_base_dir=\"/opt2/sfb_ocean/suntans/runs\"\n",
    "    hydro_timestamps=None\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ptm_run_patt=os.path.join(self.ptm_base_dir,\"chunk??\",\"20??????\")\n",
    "        self.sun_patt=os.path.join(self.sun_base_dir,\"merged_022_20??????\")\n",
    "        # does take a second\n",
    "        self.init_hydro_timestamps()\n",
    "    def __getstate__(self):\n",
    "        d=self.__dict__.copy()\n",
    "        try:\n",
    "            del d['_memocache']\n",
    "        except KeyError:\n",
    "            print(\"No memocache to avoid\")\n",
    "        return d\n",
    "\n",
    "    @memoize.imemoize()\n",
    "    def run_paths(self):\n",
    "        runs=glob.glob(self.ptm_run_patt)\n",
    "        runs.sort()\n",
    "        return runs\n",
    "    @memoize.imemoize()\n",
    "    def sun_paths(self):\n",
    "        sun_runs=glob.glob(self.sun_patt)\n",
    "        sun_runs.sort()\n",
    "        return sun_runs\n",
    "    @memoize.imemoize()\n",
    "    def grid(self):\n",
    "        \"\"\"\n",
    "        To minimize the amount of data required, just use\n",
    "        the ptm average output, rather than loading the\n",
    "        suntans input.\n",
    "        \"\"\"\n",
    "        hydro_path=self.sun_paths()[0]\n",
    "        ptm_ds=xr.open_dataset(os.path.join(hydro_path,\"ptm_average.nc_0000.nc\"))\n",
    "        g=unstructured_grid.UnstructuredGrid.read_ugrid(ptm_ds,dialect='fishptm')\n",
    "        ptm_ds.close() # safe?   \n",
    "        return g\n",
    "        \n",
    "    def runs(self,criteria=None):\n",
    "        result=[Run(path=p,experiment=self) \n",
    "                for p in self.run_paths()]\n",
    "        if criteria is not None:\n",
    "            result=[r for r in result if r.satisfies(criteria)]\n",
    "        print(\"Narrowed to %d runs\"%(len(result)))\n",
    "        return result\n",
    "    def runs_bag(self,*a,**k):\n",
    "        return db.from_sequence(self.runs(*a,**k))\n",
    "    \n",
    "    def runs_map_reduce(self,\n",
    "                        map_func=lambda r: r,\n",
    "                        reduce_func=lambda values: values,\n",
    "                        criteria=None):\n",
    "        \n",
    "        return reduce_func([map_func(r) \n",
    "                            for r in utils.progress(self.runs(criteria=criteria),\n",
    "                                                    msg=\"Runs %s\")])\n",
    "    def runs_map_reduce_bag(self,\n",
    "                            map_func=lambda r: r,\n",
    "                            reduce_func=None,\n",
    "                            criteria=None):\n",
    "        \"\"\"\n",
    "        dask version of runs_map_reduce.\n",
    "        note that reduce_func should be a dask operation, or regular function\n",
    "        that has been delayed.\n",
    "        \"\"\"\n",
    "        b=self.runs_bag(criteria=criteria)\n",
    "        b2=b.map(map_func)\n",
    "        if reduce_func is not None:\n",
    "            return reduce_func(b2)\n",
    "        else:\n",
    "            return b2\n",
    "    def all_group_names_bag(self):\n",
    "        def mf(r):\n",
    "            return r.group_names()\n",
    "        def rf(lol):\n",
    "            return [x for l in lol for x in l]\n",
    "        return self.runs_map_reduce_bag(map_func=mf,\n",
    "                                        reduce_func=dask.delayed(rf))\n",
    "    def all_groups_bag(self):\n",
    "        def mf(r):\n",
    "            grps=r.groups()\n",
    "            return grps\n",
    "        def rf(lol):    \n",
    "            return [x for l in lol for x in l]\n",
    "        return self.runs_map_reduce_bag(map_func=mf,\n",
    "                                        reduce_func=dask.delayed(rf))\n",
    "    \n",
    "    def query_particles(self,criteria):\n",
    "        \"\"\"\n",
    "        criteria: t_min, t_max ~ np.datetime64\n",
    "          behaviors: list of strings, 'down5000','none',etc.\n",
    "          sources: list of strings, shortened names.\n",
    "        returns dask dataframe.\n",
    "        \"\"\"\n",
    "        if 1:\n",
    "            # combine the query and any processing that can still be\n",
    "            # done on individual tables of particles.\n",
    "            run_bag=self.runs_bag(criteria=criteria)\n",
    "            \n",
    "            def query_process(run):\n",
    "                particles=run.query_particles(criteria)\n",
    "                self.add_particle_attrs(particles,inplace=True)\n",
    "                particles_vert=self.select_by_vertical(particles,criteria)\n",
    "                return particles_vert\n",
    "                        \n",
    "            part_bag=run_bag.map(query_process)\n",
    "            part_dds=part_bag.map(dd.from_pandas,npartitions=1)\n",
    "            part_dds_comp=part_dds.compute()\n",
    "            part_vert_sel=dd.concat(part_dds_comp)\n",
    "            \n",
    "        else: # older way\n",
    "            def query_run(run):\n",
    "                print(f\"query_particles.query_run({run.path})\")\n",
    "                return run.query_particles(criteria)        \n",
    "\n",
    "            particles=self.runs_map_reduce_bag(criteria=criteria,\n",
    "                                               map_func=query_run,\n",
    "                                               reduce_func=None)\n",
    "\n",
    "            # have to specify npartitions or chunksize\n",
    "            # keep same number of partitions.\n",
    "            part_dds=particles.map(dd.from_pandas,npartitions=1)\n",
    "            self.part_dds=part_dds # for debugging. \n",
    "            # have to compute the part_dds first\n",
    "            part_dds=part_dds.compute()\n",
    "\n",
    "            # This is still a dask dataframe, and will need to be computed,\n",
    "            # but can break it out into tasks for the next steps.\n",
    "            parts=dd.concat(part_dds)\n",
    "\n",
    "            # dask needs to know some dataframe metadata, but it can't figure \n",
    "            # it out automatically due to asserts in our code.\n",
    "            # So specify manually.\n",
    "            part_meta=dict(parts.dtypes)\n",
    "            part_meta['cell']=np.dtype('i4')\n",
    "            part_meta['z_bed']=np.dtype('f8')\n",
    "            part_meta['z_surface']=np.dtype('f8')\n",
    "\n",
    "            part_with_attrs=parts.map_partitions(self.add_particle_attrs,\n",
    "                                                 meta=part_meta)\n",
    "\n",
    "            part_vert_sel=part_with_attrs.map_partitions(self.select_by_vertical,criteria)\n",
    "        \n",
    "        return part_vert_sel\n",
    "    \n",
    "    def select_by_vertical(self,particles,criteria):\n",
    "        \"\"\"\n",
    "        Return a subset of particles based on vertical filtering\n",
    "        criteria: z_above_bed_max\n",
    "                  z_below_surface_max\n",
    "        (those would both take a positive value)\n",
    "        \"\"\"\n",
    "        if 'z_above_bed_max' in criteria:\n",
    "            sel=(particles['x2'].values-particles['z_bed'].values)<criteria['z_above_bed_max']\n",
    "            particles=particles.iloc[sel,:]\n",
    "        if 'z_below_surface_max' in criteria:\n",
    "            sel=(particles['z_surface'].values-particles['x2'].values)<criteria['z_below_surface_max']\n",
    "            particles=particles.iloc[sel,:]\n",
    "        return particles\n",
    "        \n",
    "    def get_particle_attrs(self,particles):\n",
    "        \"\"\"\n",
    "        Create a dataframe with cell, z_bed, z_surface from given particles.\n",
    "        particles should be a pandas dataframe (not dask)\n",
    "        \"\"\"\n",
    "        cell=self.get_cells(particles)\n",
    "        z_bed=self.grid().cells['z_bed'][cell]\n",
    "        z_surface=self.get_z_surface(particles,cell)\n",
    "        part_attrs=pd.DataFrame(dict(cell=cell,z_bed=z_bed,z_surface=z_surface))\n",
    "        return part_attrs\n",
    "    def add_particle_attrs(self,particles,inplace=False):\n",
    "        \"\"\"\n",
    "        Calculate a pd.dataframe with cell, z_bed, z_surface from given particles.\n",
    "        particles should be a pandas dataframe (not dask).\n",
    "        Copies particles and adds the new columns\n",
    "        \"\"\"\n",
    "        if not inplace:\n",
    "            particles=particles.copy()\n",
    "        # we have no indexes. Could either create indexes, or just assign\n",
    "        # new columns since we know they are aligned.\n",
    "        particles['cell']=cell=self.get_cells(particles)\n",
    "        particles['z_bed']=self.grid().cells['z_bed'][cell]\n",
    "        particles['z_surface']=self.get_z_surface(particles,cell)\n",
    "        \n",
    "        return particles\n",
    "    def get_cells(self,df,fallback=True):\n",
    "        \"\"\"\n",
    "        return array of cell values for the given dataframe, based on\n",
    "        coordinates in df['x0'], df['x1'], and self.grid.\n",
    "        \n",
    "        fallback: if True, particles that appear to fall outside the grid\n",
    "        will get the closest cell.\n",
    "        \"\"\"\n",
    "        grid=self.grid()\n",
    "        pnts=df[['x0','x1']].values\n",
    "        # print(\"add_cells: pnts=\",pnts)\n",
    "        cell=grid.points_to_cells(pnts)\n",
    "        \n",
    "        if fallback:\n",
    "            missing=(cell<0)\n",
    "            print(\"%d/%d cells not found on first try\"%(missing.sum(),len(missing)))\n",
    "            cell[missing]=grid.points_to_cells(pnts[missing],method='cells_nearest')\n",
    "        return cell\n",
    "    \n",
    "    def open_hydro_ds(self,idx):\n",
    "        return xr.open_dataset(os.path.join(self.sun_paths()[idx],\n",
    "                                            \"ptm_average.nc_0000.nc\"))\n",
    "\n",
    "    def init_hydro_timestamps(self):\n",
    "        N=len(self.sun_paths())\n",
    "\n",
    "        self.hydro_timestamps=np.zeros((N,2), \"<M8[ns]\") \n",
    "        # For starters, just scan the list to populate time stamps.\n",
    "        for run_idx in range(N):\n",
    "            ds=self.open_hydro_ds(run_idx)\n",
    "            t=ds.Mesh2_data_time\n",
    "            self.hydro_timestamps[run_idx,0]=t.values[0]\n",
    "            self.hydro_timestamps[run_idx,1]=t.values[-1]\n",
    "            ds.close()\n",
    "\n",
    "    def time_to_hydro_index(self,t):\n",
    "        N=self.hydro_timestamps.shape[0]\n",
    "        # search on ending to avoid one-off correction.\n",
    "        idx=np.searchsorted(self.hydro_timestamps[:,1],t).clip(0,N-1)\n",
    "        # These get in the way of inferring metadata.\n",
    "        #assert t>=self.hydro_timestamps[idx,0]\n",
    "        #assert t<=self.hydro_timestamps[idx,1]\n",
    "        if t<self.hydro_timestamps[idx,0] or t>self.hydro_timestamps[idx,1]:\n",
    "            idx=9999999999 # force bounds issue\n",
    "        return idx\n",
    "\n",
    "    def get_z_surface(self,particles,cell):\n",
    "        \"\"\"\n",
    "        particles: dataframe with 'time'\n",
    "        cell: cell index data\n",
    "         return 'z_surface' as an array with the elevation of the\n",
    "        water surface at the particle time, linearly interpolated.\n",
    "\n",
    "        It is possible but rare for particle['z'] values to be\n",
    "        above z_surface (and below z_bed) but so far this does not\n",
    "        appear to be a bug in this code. Rather, the cell mapping\n",
    "        code is approximate when a point is not clearly within any\n",
    "        cell polygon.  And in a few cases it appears the particle\n",
    "        isn't in the domain.\n",
    "        \"\"\"\n",
    "        p_time=particles['time'].values\n",
    "        p_order=np.argsort(p_time)\n",
    "        p_order_time=p_time[p_order]\n",
    "        p_order_cell=cell[p_order]\n",
    "\n",
    "        t_min=p_time.min()\n",
    "        t_max=p_time.max()\n",
    "\n",
    "        # def hydro_ds(self,t):\n",
    "        idx0=self.time_to_hydro_index(t_min)\n",
    "        idxN=self.time_to_hydro_index(t_max)\n",
    "\n",
    "        # hold the results, constructed in time order\n",
    "        p_order_eta=np.nan*np.zeros(len(particles),np.float32)\n",
    "\n",
    "        def extract_eta(ds,p_time,p_cell):\n",
    "            \"\"\"\n",
    "            ds: dataset for a chunk of a hydro run\n",
    "            p_time: times, must fall within ds\n",
    "            p_cell: cells corresponding to the particle times\n",
    "\n",
    "            returns: eta values interpolated from ds at the given times, and\n",
    "            extracted for the given cells.\n",
    "            \"\"\"\n",
    "            ds_time=ds.Mesh2_data_time.values\n",
    "            p_eta=np.nan*np.ones(len(p_time))\n",
    "            # waterlevel is instantaneous, and we can linearly interpolate\n",
    "            # between timesteps. Particles are output at 0.25h or 0.5h intervals,\n",
    "            # so it doesn't take too many different interpolations.\n",
    "            for t_grp,part_idxs in utils.progress(utils.enumerate_groups(p_time)):\n",
    "                tA=ds_time.searchsorted(t_grp)\n",
    "                assert tA<len(ds_time),\"Seems to step off the end of the data\"\n",
    "                # For an exact match, tA is the step we want.\n",
    "                if ds_time[tA]==t_grp: # good\n",
    "                    eta=ds.Mesh2_sea_surface_elevation.isel(nMesh2_data_time=tA).values\n",
    "                else:\n",
    "                    # Linearly interpolate:\n",
    "                    assert tA+1<len(ds_time)\n",
    "                    etaA=ds.Mesh2_sea_surface_elevation.isel(nMesh2_data_time=tA).values\n",
    "                    etaB=ds.Mesh2_sea_surface_elevation.isel(nMesh2_data_time=tA+1).values\n",
    "                    theta=(t_grp-ds_time[tA])/(ds_time[tA+1] - ds_time[tA])\n",
    "                    assert theta>=0\n",
    "                    assert theta<=1.0\n",
    "                    print(f\"Theta: {theta}\")\n",
    "                    eta=(1-theta)*etaA[0,:] + theta*etaB\n",
    "                p_eta[part_idxs]=eta[p_cell[part_idxs]]\n",
    "            return p_eta\n",
    "\n",
    "        for idx in range(idx0,idxN+1):\n",
    "            # Iterate over hydro runs\n",
    "            # Bad naming of variables.\n",
    "            idx_start,idx_stop=self.hydro_timestamps[idx]\n",
    "\n",
    "            # Edge cases?\n",
    "            #  for particles times [x,x,x,y,y,y,z,z,z]\n",
    "            #  and idx_start=x, idx_stop=z\n",
    "            #  particle time falls on the first step of output:\n",
    "            #  then order_start will \n",
    "            order_start=p_order_time.searchsorted(idx_start,side='left')\n",
    "            order_stop =p_order_time.searchsorted(idx_stop,side='right')\n",
    "            if order_start==order_stop:\n",
    "                print(\"Empty slice\")\n",
    "                continue\n",
    "\n",
    "            part_slc=slice(order_start,order_stop)\n",
    "            assert np.all( p_order_time[part_slc]>=idx_start )\n",
    "            assert np.all( p_order_time[part_slc]<=idx_stop  )\n",
    "\n",
    "            ds=self.open_hydro_ds(idx)\n",
    "\n",
    "            eta_slc=extract_eta(ds,\n",
    "                                p_order_time[part_slc],\n",
    "                                p_order_cell[part_slc])\n",
    "            p_order_eta[part_slc]=eta_slc\n",
    "        assert np.all(np.isfinite(p_order_eta))\n",
    "        # Undo the sort to assign back to the particles:\n",
    "        z_surface=p_order_eta[np.argsort(p_order)]\n",
    "        return z_surface\n",
    "                               \n",
    "                                \n",
    "    @memoize.imemoize()\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load loading data, return an xr.Dataset that maps\n",
    "        sources and behaviors to concentrations in particles/liter\n",
    "        \"\"\"\n",
    "        loads_orig=xr.open_dataset(\"../loads/plastic_loads-7classes-v05.nc\")\n",
    "\n",
    "        # And the per-watershed scaling factor\n",
    "        storm_scales=pd.read_csv(\"../loads/stormwater_concs-v02.csv\")\n",
    "\n",
    "        # Get rid of the extra fields in loads -- otherwise we have to fabricate\n",
    "        # them for the watersheds\n",
    "        loads=loads_orig.copy()\n",
    "\n",
    "        for fld in ['n_blank_particles','n_blank_samples','blank_rate',\n",
    "                    'total_volume','n_samples','count_w_s','count_no_w_s',\n",
    "                    'conc_raw','conc_noclip','conc_blank','blank_derate']:\n",
    "            del loads[fld]\n",
    "\n",
    "        # Extend loads_orig with storm_scales\n",
    "        storm_conc=loads_orig['conc'].sel(source='stormwater')\n",
    "        storm_conc_net=storm_conc.values.sum()\n",
    "        storm_conc_net # 8.50 particles/l\n",
    "\n",
    "        watershed_conc=np.zeros( (len(storm_scales),loads_orig.dims['category'],loads_orig.dims['w_s']))\n",
    "        # bui(source, category, w_s) \n",
    "        for storm_i,storm_scale in storm_scales.iterrows():\n",
    "            watershed_conc[storm_i,:,:] = storm_conc.values * storm_scale['net_coeff_scaled'] / storm_conc_net\n",
    "\n",
    "        watershed_loads=xr.Dataset()\n",
    "        watershed_loads['source']=('source',),storm_scales['source']\n",
    "        watershed_loads['conc']=('source','category','w_s'),watershed_conc\n",
    "        watershed_loads['source_pathway']=('source',),['stormwater']*len(storm_scales)\n",
    "        watershed_loads['pathway']=loads_orig['pathway']\n",
    "\n",
    "        loads=xr.concat( (loads,watershed_loads), dim='source')\n",
    "\n",
    "        # drop 'stormwater' to catch potential bugs later\n",
    "        return loads.isel(source=(loads.source!='stormwater'))\n",
    "\n",
    "    @memoize.imemoize()\n",
    "    def bc_ds(self):\n",
    "        \"\"\" Extract the relevant parts of the BC data, return as a single dataset\n",
    "        \"\"\"\n",
    "        compiled_fn=os.path.join(self.sun_paths()[0],'bc_extracted_v4.nc')\n",
    "\n",
    "        if not os.path.exists(compiled_fn):\n",
    "            self.extract_bc_ds(compiled_fn)\n",
    "        ds=xr.open_dataset(compiled_fn)\n",
    "        return ds\n",
    "\n",
    "    def extract_bc_ds(self,compiled_fn):\n",
    "        dss=[]\n",
    "        dss_orig=[] # for closing\n",
    "\n",
    "        for run_path in self.sun_paths():\n",
    "            # only care about point sources, and river inflows (i.e. ignore\n",
    "            # ocean flux BCs, and any freesurface BCs\n",
    "            model=sun_driver.SuntansModel.load(run_path)\n",
    "            ds=model.load_bc_ds()\n",
    "            dss_orig.append(ds)\n",
    "            ti_start,ti_stop=np.searchsorted(ds.time.values,[model.run_start,model.run_stop])\n",
    "            ds=ds.isel(Nt=slice(ti_start,ti_stop+1))\n",
    "\n",
    "            for extra in ['T','S','h','boundary_h','boundary_w','boundary_T',\n",
    "                          'boundary_u','boundary_v','z',\n",
    "                          'point_S','point_T','cellp','xv','yv','uc','vc','wc']:\n",
    "                if extra in ds: del ds[extra]\n",
    "\n",
    "            type2_sel= ds['boundary_S'].isel(Nk=0,Nt=0)==0.0\n",
    "            ds=ds.isel(Ntype2=type2_sel)\n",
    "            del ds['boundary_S']\n",
    "            dss.append(ds)\n",
    "\n",
    "        trim_dss=[]\n",
    "        for ds in dss:\n",
    "            if not trim_dss:\n",
    "                trim_dss.append(ds)\n",
    "                continue\n",
    "            else:\n",
    "                t_sel=ds.time.values>trim_dss[-1].time.values[-1]\n",
    "                if t_sel.sum():\n",
    "                    trim_dss.append(ds.isel(Nt=t_sel))\n",
    "                else:\n",
    "                    log.warning(\"BC dataset had no useful times?\")\n",
    "\n",
    "        assert len(trim_dss)\n",
    "        ds=xr.concat(trim_dss,dim='Nt',data_vars='different')\n",
    "        # somehow there is some 1e-9 difference between xe \n",
    "        for v in ['xe','ye']:\n",
    "            if 'Nt' not in ds[v].dims: continue\n",
    "            # make sure it's not a terrible issue\n",
    "            assert ds[v].std(dim='Nt').max()<1.0\n",
    "            ds[v]=ds[v].isel(Nt=0)\n",
    "\n",
    "        # A bit goofy, but try to avoid a race condition, short of\n",
    "        # utilizing a real lock.\n",
    "        tmp_fn=compiled_fn+\".\"+str(os.getpid())\n",
    "        ds.to_netcdf(tmp_fn)\n",
    "        if not os.path.exists(compiled_fn):\n",
    "            os.rename(tmp_fn,compiled_fn)\n",
    "        else:\n",
    "            log.warning(\"Things got racy while transcribing BC data\")\n",
    "            os.unlink(tmp_fn)\n",
    "\n",
    "        [ds.close() for ds in dss_orig]\n",
    "\n",
    "    @memoize.imemoize()\n",
    "    def source_Qdata(self,source):\n",
    "        \"\"\"\n",
    "        returns a time series DataArray for the\n",
    "        respective source's flow rate.\n",
    "        This step also clamps flows to be non-negative.\n",
    "        \"\"\"\n",
    "        bc_ds=self.bc_ds()\n",
    "\n",
    "        # get a flow time series for this specific group\n",
    "        try:\n",
    "            seg_i=list(bc_ds.seg_name.values).index(source)\n",
    "            Q_time_series=bc_ds.set_coords('time')['boundary_Q'].isel(Nseg=seg_i)\n",
    "        except ValueError:\n",
    "            # point sources are labeled as srcNNN\n",
    "            pnt_i=int(source.replace('src',''))\n",
    "            Q_time_series=bc_ds.set_coords('time')['point_Q'].isel(Npoint=pnt_i)\n",
    "            \n",
    "        return Q_time_series.clip(0.0)\n",
    "        \n",
    "class Run:\n",
    "    experiment=None\n",
    "    path=None\n",
    "    def __init__(self,**kw):\n",
    "        utils.set_keywords(self,kw)\n",
    "    def group_names(self):\n",
    "        bin_out_fns=glob.glob(os.path.join(self.path,\"*_bin.out\"))\n",
    "        group_names=[os.path.basename(p).replace('_bin.out','') for p in bin_out_fns]\n",
    "        return group_names\n",
    "    @memoize.imemoize()\n",
    "    def groups(self):\n",
    "        return [Group(run=self,name=name) for name in self.group_names()]\n",
    "    def satisfies(self,criteria):\n",
    "        if 't_min' in criteria and self.t_max()<criteria['t_min']:\n",
    "            return False\n",
    "        if 't_max' in criteria and self.t_min()>criteria['t_max']:\n",
    "            return False\n",
    "        return True\n",
    "    @memoize.imemoize()\n",
    "    def ptm_config(self):\n",
    "        return ptm_config.PtmConfig.load(self.path)\n",
    "    def t_max(self):\n",
    "        \"Maximum time of particle outputs for the run.\"\n",
    "        return self.ptm_config().end_time\n",
    "    def t_min(self):\n",
    "        \"Minimum time of particle outputs.\"\n",
    "        # relies on knowing that all groups follow the same start/end \n",
    "        # within a run\n",
    "        return self.groups()[0].t_min()\n",
    "    def query_particles(self,criteria):\n",
    "        results=[g.query_particles(criteria)\n",
    "                 for g in utils.progress(self.groups(),msg=\"  Groups %s\")\n",
    "                 if g.satisfies(criteria)]\n",
    "        if results:\n",
    "            df=pd.concat(results)\n",
    "            df['run']=self.path\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Try non-instance memoize to manage cache better.\n",
    "@memoize.memoize(lru=20)\n",
    "def ptm_bin(fn):\n",
    "    return ptm_tools.PtmBin(fn=fn)\n",
    "\n",
    "class Group:\n",
    "    run=None\n",
    "    name=None\n",
    "    def __init__(self,**kw):\n",
    "        utils.set_keywords(self,kw)\n",
    "        m=re.match(r'(.*)_((up|down|none)\\d*)_rel(\\d+)',self.name)\n",
    "        self.source,self.behavior,self.release_date=m.group(1),m.group(2),m.group(4)\n",
    "    def satisfies(self,criteria):\n",
    "        if 'sources' in criteria and self.source not in criteria.sources:\n",
    "            return False\n",
    "        if 't_min' in criteria and self.t_max() < criteria['t_min']:\n",
    "            return False\n",
    "        if 't_max' in criteria and self.t_min() > criteria['t_max']:\n",
    "            return False\n",
    "        if 'behaviors' in criteria and self.behavior not in criteria['behaviors']:\n",
    "            return False\n",
    "        return True\n",
    "    @memoize.imemoize()\n",
    "    def t_max(self):\n",
    "        return self.run.t_max()\n",
    "    @memoize.imemoize()\n",
    "    def t_min(self):\n",
    "        release_log_fn=os.path.join(self.run.path,self.name+\".release_log\")\n",
    "        with open(release_log_fn,'rt') as fp:\n",
    "            rel_date,rel_time=fp.readline().strip().split()[-2:]\n",
    "        return np.datetime64(rel_date+\" \"+rel_time)\n",
    "    def ptm_bin(self):\n",
    "        return ptm_bin(os.path.join(self.run.path,self.name+\"_bin.out\"))\n",
    "    @memoize.imemoize()\n",
    "    def release_log(self):\n",
    "        fn=os.path.join(self.run.path,self.name+\".release_log\")\n",
    "        # df_rel=ptm_tools.release_log_dataframe(fn=fn)\n",
    "        rel=ptm_tools.ReleaseLog(fn)\n",
    "        df_rel=rel.data\n",
    "        # release has id and gid. So far they are always the same\n",
    "        # Trust, but verify.\n",
    "        assert np.all(df_rel['id'].values==df_rel['gid'].values),\"Invariant check\"\n",
    "        return rel\n",
    "    \n",
    "    def query_particles(self,criteria):\n",
    "        # Could be faster by using index instead of bin file\n",
    "        pbf=self.ptm_bin()\n",
    "        times=np.array( [utils.to_dt64(t) for t in pbf.time] )\n",
    "        sel=np.ones(len(times),np.bool8)\n",
    "        if 't_min' in criteria:\n",
    "            sel=sel&(times>=criteria['t_min'])\n",
    "        if 't_max' in criteria:\n",
    "            sel=sel&(times<=criteria['t_max'])\n",
    "\n",
    "        all_particles=[]\n",
    "        for ts in np.nonzero(sel)[0]:\n",
    "            datenum,particles=pbf.read_timestep(ts)\n",
    "            t=utils.to_dt64(datenum)\n",
    "            #particles=self.filter_particles(particle,criteria)\n",
    "            #df=pd.DataFrame(particles) # Can't deal with vectors\n",
    "            df=pd.DataFrame()\n",
    "            df['id']=particles['id']\n",
    "            df['x0']=particles['x'][:,0]\n",
    "            df['x1']=particles['x'][:,1]\n",
    "            df['x2']=particles['x'][:,2]\n",
    "            df['active']=particles['active']\n",
    "            # Avoid keeping all this memmap'd data around\n",
    "            del particles\n",
    "            df['time']=t\n",
    "            all_particles.append(df)\n",
    "        if len(all_particles):\n",
    "            df=pd.concat(all_particles)\n",
    "            df['group']=self.name\n",
    "            \n",
    "            # Assign release times\n",
    "            # If this is slow, could verify that release_log index \n",
    "            # is dense, and do this in numpy\n",
    "            df_rel=self.release_log().data.set_index('id') # id vs gid?\n",
    "            rel_times=df_rel.loc[ df['id'].values,'date_time']\n",
    "            df['rel_time']=rel_times.values\n",
    "            \n",
    "            self.add_mp_count(df,criteria)\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    def w_s(self):\n",
    "        w_s_map={'down50000':0.05,\n",
    "                 'down5000':0.005,\n",
    "                 'down500':0.0005,\n",
    "                 'none':0.0,\n",
    "                 'up500':0.0005,\n",
    "                 'up5000':0.005,\n",
    "                 'up50000':0.05}\n",
    "        return w_s_map[self.behavior]\n",
    "    \n",
    "    # For a subset of the source names in the PTM setup,\n",
    "    # map them to the name used in the loading data\n",
    "    source_ptm_to_load={\n",
    "        'cccsd':'CCCSD',\n",
    "        'sunnyvale':'SUNN',\n",
    "        'fs':'FSSD',\n",
    "        'palo_alto':'PA',\n",
    "        'san_jose':'SJ',\n",
    "        'src000':'EBDA',\n",
    "        'src001':'EBMUD',\n",
    "        'src002':'SFPUC',\n",
    "        # These shouldn't be used, but including just to be sure\n",
    "        # that if they somehow show up, they won't contaminate\n",
    "        # stormwater.\n",
    "        'SacRiver':'DELTA',\n",
    "        'SJRiver':'DELTA'\n",
    "    }\n",
    "\n",
    "    def load_conc(self,categories='all'):\n",
    "        \"\"\"\n",
    "        Concentration for this particle group, in particles/liter.\n",
    "        categories: 'all','nonfiber', a list of category names,\n",
    "        or a single category name\n",
    "        \"\"\"\n",
    "        load_data=self.run.experiment.load_data()\n",
    "        # source: e.g. 'San_Lorenzo_C'\n",
    "        if self.source in self.source_ptm_to_load:\n",
    "            load_source=self.source_ptm_to_load[self.source]\n",
    "        else:\n",
    "            load_source=self.source\n",
    "            \n",
    "        conc_over_category=load_data['conc'].sel(source=load_source,w_s=self.w_s())\n",
    "        if categories=='all':\n",
    "            conc=conc_over_category.values.sum()\n",
    "        elif categories=='nonfiber':\n",
    "            sel=conc_over_category.category!='Fiber'\n",
    "            conc=conc_over_category.isel(category=sel).values.sum()\n",
    "        else:\n",
    "            conc=conc_over_category.sel(category=categories).values.sum()\n",
    "        return conc\n",
    "    \n",
    "    def Qdata(self):\n",
    "        return self.run.experiment.source_Qdata(self.source)\n",
    "\n",
    "    def volume_per_particle(self,particles):\n",
    "        \"\"\"\n",
    "        Need particles/per time for release time t\n",
    "        and volume/time for source at time t.\n",
    "        Returns array with volume per particle for each of the given particles.\n",
    "\n",
    "        Some parts of this aren't really group specific, could be moved elsewhere.\n",
    "        \"\"\"\n",
    "        # Sanity check:\n",
    "        # this is giving about 60m3/particle.\n",
    "        # I know there are 10 particles/hr released.\n",
    "        # so that's saying that San_Lorenzo_C[reek] has\n",
    "        # a flow of 600m3/hour, or 0.17 m3/s in June 2017.\n",
    "        # USGS measured is 6-8 cfs, which is order 0.2 m3/s.\n",
    "        # Good.\n",
    "\n",
    "        # pbf=run.open_binfile(group)\n",
    "        rel=self.release_log()\n",
    "\n",
    "        # Record the data from release_log\n",
    "        release=rel.intervals\n",
    "        # release['group_id']=group_id\n",
    "        # release['epoch']=(release['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "        # Ham-handed fix up of truncated release logs.\n",
    "        typical_count=int( np.median( release['count'] ) )\n",
    "        bad=release['count']!=typical_count\n",
    "        # a little bit smart -- only step in when it's the last step that's\n",
    "        # different.\n",
    "        if (not np.any(bad[:-1])) and bad[-1]:\n",
    "            print(\"Yikes - release_log might be missing some particles.\")\n",
    "            print(\"  Changing reported count of %d to %d\"%(release['count'].values[-1],\n",
    "                                                           typical_count))\n",
    "            print(\"  And punting on id_max,gid_max\")\n",
    "            release.loc[bad,'count']=typical_count\n",
    "            # careful of inclusive indexes\n",
    "            release.loc[bad,'id_max']=release.loc[bad,'id_min']+typical_count-1\n",
    "            release.loc[bad,'gid_max']=release.loc[bad,'gid_min']+typical_count-1\n",
    "\n",
    "        # add in volume information.\n",
    "        Qdata=self.Qdata() \n",
    "        Qvalues=Qdata.values\n",
    "        Qpart=np.zeros(len(particles),np.float64)\n",
    "        Qpart[:]=np.nan # for safety\n",
    "        # This could be sped up since the same time will appear many times\n",
    "        # in particles. Does it matter?\n",
    "        for t,part_idxs in utils.enumerate_groups(particles['rel_time'].values):\n",
    "            Qidx=np.searchsorted(Qdata.time.values,t)\n",
    "            Qpart[part_idxs]=Qvalues[Qidx]\n",
    "\n",
    "        grp_hour_per_rel=(release['time'].values[1] - release['time'].values[0])/np.timedelta64(3600,'s')\n",
    "\n",
    "        # m3/s * s/hour * hour/release / (particles/release) => m3/particle\n",
    "        part_volume=Qpart*3600 * grp_hour_per_rel/typical_count\n",
    "        return part_volume # m3/particle\n",
    "    def add_mp_count(self,particles,criteria):\n",
    "        category=criteria.get('category','all')\n",
    "        mp_per_liter=self.load_conc(category)\n",
    "\n",
    "        # how many m3 of effluent does each computational particle\n",
    "        # represent?\n",
    "        m3_per_particle=self.volume_per_particle(particles)\n",
    "        #                  mp_count / m3   *   m3 / particle\n",
    "        mp_per_particle=mp_per_liter * 1e3 * m3_per_particle\n",
    "        particles['mp_count']=mp_per_particle\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narrowed to 20 runs\n",
      "9.449 GB maxrss\n",
      "16185743 particles found\n",
      "10.791 GB maxrss\n",
      "CPU times: user 18.9 s, sys: 5.75 s, total: 24.7 s\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>active</th>\n",
       "      <th>time</th>\n",
       "      <th>group</th>\n",
       "      <th>rel_time</th>\n",
       "      <th>mp_count</th>\n",
       "      <th>run</th>\n",
       "      <th>cell</th>\n",
       "      <th>z_bed</th>\n",
       "      <th>z_surface</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>485365.581868</td>\n",
       "      <td>4.197739e+06</td>\n",
       "      <td>-4.307611</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-01 20:00:00</td>\n",
       "      <td>Pacheco_Creek_up5000_rel20170620</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>130326.830618</td>\n",
       "      <td>/opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...</td>\n",
       "      <td>49324</td>\n",
       "      <td>-103.324100</td>\n",
       "      <td>-4.249202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>541056.328566</td>\n",
       "      <td>4.155373e+06</td>\n",
       "      <td>-4.279968</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-01 12:00:00</td>\n",
       "      <td>Guadalupe_Slo_up50000_rel20170620</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>19634.497352</td>\n",
       "      <td>/opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...</td>\n",
       "      <td>51434</td>\n",
       "      <td>-29.389260</td>\n",
       "      <td>-4.273002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>556159.126783</td>\n",
       "      <td>4.167180e+06</td>\n",
       "      <td>-3.756978</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-01 05:00:00</td>\n",
       "      <td>palo_alto_none_rel20170620</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>/opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...</td>\n",
       "      <td>11754</td>\n",
       "      <td>-9.312405</td>\n",
       "      <td>-3.612916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>544643.573220</td>\n",
       "      <td>4.192698e+06</td>\n",
       "      <td>-3.745396</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-01 00:00:00</td>\n",
       "      <td>Coyote_Creek__up5000_rel20170620</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>251803.990392</td>\n",
       "      <td>/opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...</td>\n",
       "      <td>30799</td>\n",
       "      <td>-6.621319</td>\n",
       "      <td>-3.612564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>544681.778296</td>\n",
       "      <td>4.217964e+06</td>\n",
       "      <td>-4.182371</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-01 14:00:00</td>\n",
       "      <td>Petaluma_Rive_none_rel20170620</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>163.954728</td>\n",
       "      <td>/opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...</td>\n",
       "      <td>23909</td>\n",
       "      <td>-5.495636</td>\n",
       "      <td>-4.165472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             x0            x1        x2  active                time  \\\n",
       "0   1  485365.581868  4.197739e+06 -4.307611       1 2017-08-01 20:00:00   \n",
       "0   1  541056.328566  4.155373e+06 -4.279968       1 2017-08-01 12:00:00   \n",
       "0   1  556159.126783  4.167180e+06 -3.756978       1 2017-08-01 05:00:00   \n",
       "0   2  544643.573220  4.192698e+06 -3.745396       1 2017-08-01 00:00:00   \n",
       "0   1  544681.778296  4.217964e+06 -4.182371       1 2017-08-01 14:00:00   \n",
       "\n",
       "                               group   rel_time       mp_count  \\\n",
       "0   Pacheco_Creek_up5000_rel20170620 2017-06-20  130326.830618   \n",
       "0  Guadalupe_Slo_up50000_rel20170620 2017-06-20   19634.497352   \n",
       "0         palo_alto_none_rel20170620 2017-06-20       0.000000   \n",
       "0   Coyote_Creek__up5000_rel20170620 2017-06-20  251803.990392   \n",
       "0     Petaluma_Rive_none_rel20170620 2017-06-20     163.954728   \n",
       "\n",
       "                                                 run   cell       z_bed  \\\n",
       "0  /opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...  49324 -103.324100   \n",
       "0  /opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...  51434  -29.389260   \n",
       "0  /opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...  11754   -9.312405   \n",
       "0  /opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...  30799   -6.621319   \n",
       "0  /opt2/sfb_ocean/ptm/all_source_022a/chunk00/20...  23909   -5.495636   \n",
       "\n",
       "   z_surface  \n",
       "0  -4.249202  \n",
       "0  -4.273002  \n",
       "0  -3.612916  \n",
       "0  -3.612564  \n",
       "0  -4.165472  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "experiment=Experiment()\n",
    "\n",
    "criteria=dict(t_min=np.datetime64(\"2017-08-01 00:00\"),\n",
    "              t_max=np.datetime64(\"2017-08-02 00:00\"),\n",
    "              category='nonfiber',\n",
    "              z_below_surface_max=0.20)\n",
    "#criteria['behaviors']=['up5000']\n",
    "\n",
    "parts=experiment.query_particles(criteria=criteria)\n",
    "print_rusage() # 2GB here\n",
    "\n",
    "particles=parts.compute()\n",
    "\n",
    "print(f\"{len(particles)} particles found\")\n",
    "print_rusage() # 3.2GB here\n",
    "\n",
    "particles.head().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status\n",
    "--\n",
    "\n",
    "Reasonable performance reading data single-threaded.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "3. How hard would it be to parallelize at some level?  Don't do this too\n",
    "   early, but take a look in case it's easy.\n",
    "4. Recreate some of the figures from before, including on-grid smoothing.\n",
    "5. Pull out manta samples as before. Maybe skip putting it on the grid, just\n",
    "   query a radius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible Parallelization\n",
    "--\n",
    "\n",
    "Where is the most time spent?\n",
    "\n",
    "Seems that looking up the cells is still pretty expensive.\n",
    "\n",
    "All that's needed for that step is the grid and particle coordinates.\n",
    "\n",
    "It could be done by groups, it could be done at the experiment level.\n",
    "\n",
    "Need to profile a large-ish operation. If there is a lot of work that's\n",
    "slow and at the group level, then parallelizing groups would be good.\n",
    "\n",
    "315s, and 83s in 2401 calls to memoizer.  Try pushing ptm_bin outside\n",
    "of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.part_dds.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rusage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(particles) # 6.2M\n",
    "particles_gb=particles.memory_usage().sum() / 1e9\n",
    "print(f\"Particles size: {particles_gb:.3f} GB\")\n",
    "\n",
    "# If I do a clean run, and do all of this same stuff,\n",
    "# what's the usage\n",
    "?# Say I only get to the part_dds, and do no more work.\n",
    "# What's my memory usage?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
