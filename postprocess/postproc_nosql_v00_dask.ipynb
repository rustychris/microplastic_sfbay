{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Approach to Non-SQL PTM Queries\n",
    "--\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.33 MB\n"
     ]
    }
   ],
   "source": [
    "# There are some issues with memory usage. It seems\n",
    "# (mostly based on github discussions) that with dask\n",
    "# shuttling many large memory allocations around, memory\n",
    "# gets fragmented and effectively lost. print_rusage()\n",
    "# will report current memory usage, and the code in the\n",
    "# next block changes malloc behavior to reduce \n",
    "# fragmentation.\n",
    "\n",
    "import gc\n",
    "import psutil\n",
    "from distributed.utils import format_bytes\n",
    "proc = psutil.Process()\n",
    "\n",
    "def print_rusage():\n",
    "    gc.collect()\n",
    "    print(format_bytes(proc.memory_info().rss))\n",
    "    #ru=resource.getrusage(resource.RUSAGE_SELF)\n",
    "    #print(f\"{ru.ru_maxrss/1000/1000.:.3f} GB maxrss\")\n",
    "    # That's saying that this process is using 55GB of RAM!?\n",
    "    \n",
    "print_rusage() # Starts 80MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ctypes import *\n",
    "libc = CDLL('libc.so.6')\n",
    "mallopt = libc.mallopt\n",
    "mallopt.argtypes = [c_int, c_int]\n",
    "mallopt.restype = c_int\n",
    " \n",
    "M_MMAP_THRESHOLD = -3\n",
    "# The following would return 0 on error\n",
    "mallopt(M_MMAP_THRESHOLD, 16*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client.close()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da495a7934aa4d69be028025e42898fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "\n",
    "from dask.distributed import Client\n",
    "client=Client(n_workers=8,threads_per_worker=1)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import re\n",
    "\n",
    "from stompy.grid import unstructured_grid\n",
    "from stompy import utils, memoize\n",
    "from stompy.model.fish_ptm import ptm_config, ptm_tools\n",
    "from stompy.model.suntans import sun_driver\n",
    "\n",
    "import stompy.plot.cmap as scmap\n",
    "turbo=scmap.load_gradient('turbo.cpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257.70 MB\n"
     ]
    }
   ],
   "source": [
    "print_rusage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Process\n",
    "===\n",
    "\n",
    "1. SUNTANS hydro runs\n",
    "2. SUNTANS average output\n",
    "3. ptm-formatted average output\n",
    "4. PTM runs\n",
    "5. Load data\n",
    "\n",
    "The top-level query is something like *generate a map of concentrations for...*\n",
    "\n",
    "filter on:\n",
    " - sources $x$\n",
    " - settling classes $y$\n",
    " - vertical positions $z$\n",
    " - horizontal positions $h$\n",
    "\n",
    "weighted by\n",
    "\n",
    " - loading data \n",
    " - age\n",
    " \n",
    "mapped by one of ...\n",
    "\n",
    " - bounding box\n",
    " - put on hydro grid\n",
    " - put on regular grid\n",
    "\n",
    "and possibly smoothed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    ptm_base_dir=\"/opt2/sfb_ocean/ptm/all_source_022a\"\n",
    "    sun_base_dir=\"/opt2/sfb_ocean/suntans/runs\"\n",
    "    hydro_timestamps=None\n",
    "    \n",
    "    # How often particle data is output. This is used to calculate\n",
    "    # how many snapshots are included in a given time range.\n",
    "    ptm_output_interval=np.timedelta64(1,'h')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ptm_run_patt=os.path.join(self.ptm_base_dir,\"chunk??\",\"20??????\")\n",
    "        self.sun_patt=os.path.join(self.sun_base_dir,\"merged_022_20??????\")\n",
    "        # does take a second\n",
    "        self.init_hydro_timestamps()\n",
    "        self.grid()\n",
    "        \n",
    "    def __getstate__(self):\n",
    "        d=self.__dict__.copy()\n",
    "        try:\n",
    "            del d['_memocache']\n",
    "        except KeyError:\n",
    "            print(\"No memocache to avoid\")\n",
    "        return d\n",
    "\n",
    "    @memoize.imemoize()\n",
    "    def run_paths(self):\n",
    "        runs=glob.glob(self.ptm_run_patt)\n",
    "        runs.sort()\n",
    "        return runs\n",
    "    @memoize.imemoize()\n",
    "    def sun_paths(self):\n",
    "        sun_runs=glob.glob(self.sun_patt)\n",
    "        sun_runs.sort()\n",
    "        return sun_runs\n",
    "    \n",
    "    _grid=None\n",
    "    def grid(self):\n",
    "        \"\"\"\n",
    "        To minimize the amount of data required, just use\n",
    "        the ptm average output, rather than loading the\n",
    "        suntans input.\n",
    "        \"\"\"\n",
    "        if self._grid is None:\n",
    "            hydro_path=self.sun_paths()[0]\n",
    "            ptm_ds=xr.open_dataset(os.path.join(hydro_path,\"ptm_average.nc_0000.nc\"))\n",
    "            self._grid=unstructured_grid.UnstructuredGrid.read_ugrid(ptm_ds,dialect='fishptm')\n",
    "            ptm_ds.close() # safe?   \n",
    "        return self._grid\n",
    "        \n",
    "        \n",
    "    def runs(self,criteria=None):\n",
    "        result=[Run(path=p,experiment=self) \n",
    "                for p in self.run_paths()]\n",
    "        if criteria is not None:\n",
    "            result=[r for r in result if r.satisfies(criteria)]\n",
    "        print(\"Narrowed to %d runs\"%(len(result)))\n",
    "        return result\n",
    "    def runs_bag(self,*a,**k):\n",
    "        return db.from_sequence(self.runs(*a,**k))\n",
    "    \n",
    "    def group_bag(self,criteria):\n",
    "        \"\"\"\n",
    "        Return a bag of groups matching the criteria\n",
    "        \"\"\"\n",
    "        run_bag=self.runs_bag(criteria=criteria)\n",
    "        bag_of_bags=run_bag.map(lambda r: r.query_groups(criteria))\n",
    "        return bag_of_bags.flatten().repartition(20*run_bag.npartitions) # just a guess.\n",
    "\n",
    "    def runs_map_reduce(self,\n",
    "                        map_func=lambda r: r,\n",
    "                        reduce_func=lambda values: values,\n",
    "                        criteria=None):\n",
    "        \n",
    "        return reduce_func([map_func(r) \n",
    "                            for r in utils.progress(self.runs(criteria=criteria),\n",
    "                                                    msg=\"Runs %s\")])\n",
    "    def runs_map_reduce_bag(self,\n",
    "                            map_func=lambda r: r,\n",
    "                            reduce_func=None,\n",
    "                            criteria=None):\n",
    "        \"\"\"\n",
    "        dask version of runs_map_reduce.\n",
    "        note that reduce_func should be a dask operation, or regular function\n",
    "        that has been delayed.\n",
    "        \"\"\"\n",
    "        b=self.runs_bag(criteria=criteria)\n",
    "        b2=b.map(map_func)\n",
    "        if reduce_func is not None:\n",
    "            return reduce_func(b2)\n",
    "        else:\n",
    "            return b2\n",
    "    def all_group_names_bag(self):\n",
    "        def mf(r):\n",
    "            return r.group_names()\n",
    "        def rf(lol):\n",
    "            return [x for l in lol for x in l]\n",
    "        return self.runs_map_reduce_bag(map_func=mf,\n",
    "                                        reduce_func=dask.delayed(rf))\n",
    "    def all_groups_bag(self):\n",
    "        def mf(r):\n",
    "            grps=r.groups()\n",
    "            return grps\n",
    "        def rf(lol):    \n",
    "            return [x for l in lol for x in l]\n",
    "        return self.runs_map_reduce_bag(map_func=mf,\n",
    "                                        reduce_func=dask.delayed(rf))\n",
    "    \n",
    "    def query_particles(self,criteria):\n",
    "        \"\"\"\n",
    "        criteria: t_min, t_max ~ np.datetime64\n",
    "          behaviors: list of strings, 'down5000','none',etc.\n",
    "          sources: list of strings, shortened names.\n",
    "        returns dask dataframe.\n",
    "        \"\"\"\n",
    "        # combine the query and any processing that can still be\n",
    "        # done on individual tables of particles.\n",
    "        if 0:\n",
    "            run_bag=self.runs_bag(criteria=criteria)\n",
    "            def query_process(run):\n",
    "                particles=run.query_particles(criteria)\n",
    "                self.add_particle_attrs(particles,inplace=True)\n",
    "                particles_vert=self.select_by_vertical(particles,criteria)\n",
    "                return particles_vert\n",
    "\n",
    "            part_bag=run_bag.map(query_process)\n",
    "        else:\n",
    "            # Better granularity with large queries:\n",
    "            group_bag=self.group_bag(criteria=criteria)\n",
    "            def do_query_group(grp):\n",
    "                particles=grp.query_particles(criteria)\n",
    "                if len(particles):\n",
    "                    self.add_particle_attrs(particles,inplace=True)\n",
    "                    particles_vert=self.select_by_vertical(particles,criteria)\n",
    "                    return particles_vert\n",
    "                else:\n",
    "                    return None\n",
    "            part_bag=group_bag.map(do_query_group)\n",
    "\n",
    "        def valid(b):\n",
    "            return b is not None\n",
    "        \n",
    "        part_dds=part_bag.filter(valid).map(dd.from_pandas,npartitions=1)\n",
    "        part_dds_comp=part_dds.compute()\n",
    "        part_vert_sel=dd.concat(part_dds_comp)\n",
    "                    \n",
    "        return part_vert_sel\n",
    "    \n",
    "    def select_by_vertical(self,particles,criteria):\n",
    "        \"\"\"\n",
    "        Return a subset of particles based on vertical filtering\n",
    "        criteria: z_above_bed_max\n",
    "                  z_below_surface_max\n",
    "        (those would both take a positive value)\n",
    "        \"\"\"\n",
    "        if 'z_above_bed_max' in criteria:\n",
    "            sel=(particles['x2'].values-particles['z_bed'].values)<criteria['z_above_bed_max']\n",
    "            particles=particles.iloc[sel,:]\n",
    "        if 'z_below_surface_max' in criteria:\n",
    "            sel=(particles['z_surface'].values-particles['x2'].values)<criteria['z_below_surface_max']\n",
    "            particles=particles.iloc[sel,:]\n",
    "        return particles\n",
    "        \n",
    "    def get_particle_attrs(self,particles):\n",
    "        \"\"\"\n",
    "        Create a dataframe with cell, z_bed, z_surface from given particles.\n",
    "        particles should be a pandas dataframe (not dask)\n",
    "        \"\"\"\n",
    "        cell=self.get_cells(particles)\n",
    "        z_bed=self.grid().cells['z_bed'][cell]\n",
    "        z_surface=self.get_z_surface(particles,cell)\n",
    "        part_attrs=pd.DataFrame(dict(cell=cell,z_bed=z_bed,z_surface=z_surface))\n",
    "        return part_attrs\n",
    "    def add_particle_attrs(self,particles,inplace=False):\n",
    "        \"\"\"\n",
    "        Calculate a pd.dataframe with cell, z_bed, z_surface from given particles.\n",
    "        particles should be a pandas dataframe (not dask).\n",
    "        Copies particles and adds the new columns\n",
    "        \"\"\"\n",
    "        if not inplace:\n",
    "            particles=particles.copy()\n",
    "        # we have no indexes. Could either create indexes, or just assign\n",
    "        # new columns since we know they are aligned.\n",
    "        particles['cell']=cell=self.get_cells(particles)\n",
    "        particles['z_bed']=self.grid().cells['z_bed'][cell]\n",
    "        particles['z_surface']=self.get_z_surface(particles,cell)\n",
    "        \n",
    "        return particles\n",
    "    def get_cells(self,df,fallback=True):\n",
    "        \"\"\"\n",
    "        return array of cell values for the given dataframe, based on\n",
    "        coordinates in df['x0'], df['x1'], and self.grid.\n",
    "        \n",
    "        fallback: if True, particles that appear to fall outside the grid\n",
    "        will get the closest cell.\n",
    "        \"\"\"\n",
    "        grid=self.grid()\n",
    "        pnts=df[['x0','x1']].values\n",
    "        # print(\"add_cells: pnts=\",pnts)\n",
    "        cell=grid.points_to_cells(pnts)\n",
    "        \n",
    "        if fallback:\n",
    "            missing=(cell<0)\n",
    "            print(\"%d/%d cells not found on first try\"%(missing.sum(),len(missing)))\n",
    "            cell[missing]=grid.points_to_cells(pnts[missing],method='cells_nearest')\n",
    "        return cell\n",
    "    \n",
    "    def open_hydro_ds(self,idx):\n",
    "        return xr.open_dataset(os.path.join(self.sun_paths()[idx],\n",
    "                                            \"ptm_average.nc_0000.nc\"))\n",
    "\n",
    "    def init_hydro_timestamps(self):\n",
    "        N=len(self.sun_paths())\n",
    "\n",
    "        self.hydro_timestamps=np.zeros((N,2), \"<M8[ns]\") \n",
    "        # For starters, just scan the list to populate time stamps.\n",
    "        for run_idx in range(N):\n",
    "            ds=self.open_hydro_ds(run_idx)\n",
    "            t=ds.Mesh2_data_time\n",
    "            self.hydro_timestamps[run_idx,0]=t.values[0]\n",
    "            self.hydro_timestamps[run_idx,1]=t.values[-1]\n",
    "            ds.close()\n",
    "\n",
    "    def time_to_hydro_index(self,t):\n",
    "        N=self.hydro_timestamps.shape[0]\n",
    "        # search on ending to avoid one-off correction.\n",
    "        idx=np.searchsorted(self.hydro_timestamps[:,1],t).clip(0,N-1)\n",
    "        # These get in the way of inferring metadata.\n",
    "        #assert t>=self.hydro_timestamps[idx,0]\n",
    "        #assert t<=self.hydro_timestamps[idx,1]\n",
    "        if t<self.hydro_timestamps[idx,0] or t>self.hydro_timestamps[idx,1]:\n",
    "            idx=9999999999 # force bounds issue\n",
    "        return idx\n",
    "\n",
    "    def get_z_surface(self,particles,cell):\n",
    "        \"\"\"\n",
    "        particles: dataframe with 'time'\n",
    "        cell: cell index data\n",
    "         return 'z_surface' as an array with the elevation of the\n",
    "        water surface at the particle time, linearly interpolated.\n",
    "\n",
    "        It is possible but rare for particle['z'] values to be\n",
    "        above z_surface (and below z_bed) but so far this does not\n",
    "        appear to be a bug in this code. Rather, the cell mapping\n",
    "        code is approximate when a point is not clearly within any\n",
    "        cell polygon.  And in a few cases it appears the particle\n",
    "        isn't in the domain.\n",
    "        \"\"\"\n",
    "        p_time=particles['time'].values\n",
    "        p_order=np.argsort(p_time)\n",
    "        p_order_time=p_time[p_order]\n",
    "        p_order_cell=cell[p_order]\n",
    "\n",
    "        t_min=p_time.min()\n",
    "        t_max=p_time.max()\n",
    "\n",
    "        # def hydro_ds(self,t):\n",
    "        idx0=self.time_to_hydro_index(t_min)\n",
    "        idxN=self.time_to_hydro_index(t_max)\n",
    "\n",
    "        # hold the results, constructed in time order\n",
    "        p_order_eta=np.nan*np.zeros(len(particles),np.float32)\n",
    "\n",
    "        def extract_eta(ds,p_time,p_cell):\n",
    "            \"\"\"\n",
    "            ds: dataset for a chunk of a hydro run\n",
    "            p_time: times, must fall within ds\n",
    "            p_cell: cells corresponding to the particle times\n",
    "\n",
    "            returns: eta values interpolated from ds at the given times, and\n",
    "            extracted for the given cells.\n",
    "            \"\"\"\n",
    "            ds_time=ds.Mesh2_data_time.values\n",
    "            p_eta=np.nan*np.ones(len(p_time))\n",
    "            # waterlevel is instantaneous, and we can linearly interpolate\n",
    "            # between timesteps. Particles are output at 0.25h or 0.5h intervals,\n",
    "            # so it doesn't take too many different interpolations.\n",
    "            for t_grp,part_idxs in utils.progress(utils.enumerate_groups(p_time)):\n",
    "                tA=ds_time.searchsorted(t_grp)\n",
    "                assert tA<len(ds_time),\"Seems to step off the end of the data\"\n",
    "                # For an exact match, tA is the step we want.\n",
    "                if ds_time[tA]==t_grp: # good\n",
    "                    eta=ds.Mesh2_sea_surface_elevation.isel(nMesh2_data_time=tA).values\n",
    "                else:\n",
    "                    # Linearly interpolate:\n",
    "                    assert tA+1<len(ds_time)\n",
    "                    etaA=ds.Mesh2_sea_surface_elevation.isel(nMesh2_data_time=tA).values\n",
    "                    etaB=ds.Mesh2_sea_surface_elevation.isel(nMesh2_data_time=tA+1).values\n",
    "                    theta=(t_grp-ds_time[tA])/(ds_time[tA+1] - ds_time[tA])\n",
    "                    assert theta>=0\n",
    "                    assert theta<=1.0\n",
    "                    print(f\"Theta: {theta}\")\n",
    "                    eta=(1-theta)*etaA[0,:] + theta*etaB\n",
    "                p_eta[part_idxs]=eta[p_cell[part_idxs]]\n",
    "            return p_eta\n",
    "\n",
    "        for idx in range(idx0,idxN+1):\n",
    "            # Iterate over hydro runs\n",
    "            # Bad naming of variables.\n",
    "            idx_start,idx_stop=self.hydro_timestamps[idx]\n",
    "\n",
    "            # Edge cases?\n",
    "            #  for particles times [x,x,x,y,y,y,z,z,z]\n",
    "            #  and idx_start=x, idx_stop=z\n",
    "            #  particle time falls on the first step of output:\n",
    "            #  then order_start will \n",
    "            order_start=p_order_time.searchsorted(idx_start,side='left')\n",
    "            order_stop =p_order_time.searchsorted(idx_stop,side='right')\n",
    "            if order_start==order_stop:\n",
    "                print(\"Empty slice\")\n",
    "                continue\n",
    "\n",
    "            part_slc=slice(order_start,order_stop)\n",
    "            assert np.all( p_order_time[part_slc]>=idx_start )\n",
    "            assert np.all( p_order_time[part_slc]<=idx_stop  )\n",
    "\n",
    "            ds=self.open_hydro_ds(idx)\n",
    "\n",
    "            eta_slc=extract_eta(ds,\n",
    "                                p_order_time[part_slc],\n",
    "                                p_order_cell[part_slc])\n",
    "            p_order_eta[part_slc]=eta_slc\n",
    "        assert np.all(np.isfinite(p_order_eta))\n",
    "        # Undo the sort to assign back to the particles:\n",
    "        z_surface=p_order_eta[np.argsort(p_order)]\n",
    "        return z_surface\n",
    "                               \n",
    "                                \n",
    "    @memoize.imemoize()\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load loading data, return an xr.Dataset that maps\n",
    "        sources and behaviors to concentrations in particles/liter\n",
    "        \"\"\"\n",
    "        loads_orig=xr.open_dataset(\"../loads/plastic_loads-7classes-v05.nc\")\n",
    "\n",
    "        # And the per-watershed scaling factor\n",
    "        storm_scales=pd.read_csv(\"../loads/stormwater_concs-v02.csv\")\n",
    "\n",
    "        # Get rid of the extra fields in loads -- otherwise we have to fabricate\n",
    "        # them for the watersheds\n",
    "        loads=loads_orig.copy()\n",
    "\n",
    "        for fld in ['n_blank_particles','n_blank_samples','blank_rate',\n",
    "                    'total_volume','n_samples','count_w_s','count_no_w_s',\n",
    "                    'conc_raw','conc_noclip','conc_blank','blank_derate']:\n",
    "            del loads[fld]\n",
    "\n",
    "        # Extend loads_orig with storm_scales\n",
    "        storm_conc=loads_orig['conc'].sel(source='stormwater')\n",
    "        storm_conc_net=storm_conc.values.sum()\n",
    "        storm_conc_net # 8.50 particles/l\n",
    "\n",
    "        watershed_conc=np.zeros( (len(storm_scales),loads_orig.dims['category'],loads_orig.dims['w_s']))\n",
    "        # bui(source, category, w_s) \n",
    "        for storm_i,storm_scale in storm_scales.iterrows():\n",
    "            watershed_conc[storm_i,:,:] = storm_conc.values * storm_scale['net_coeff_scaled'] / storm_conc_net\n",
    "\n",
    "        watershed_loads=xr.Dataset()\n",
    "        watershed_loads['source']=('source',),storm_scales['source']\n",
    "        watershed_loads['conc']=('source','category','w_s'),watershed_conc\n",
    "        watershed_loads['source_pathway']=('source',),['stormwater']*len(storm_scales)\n",
    "        watershed_loads['pathway']=loads_orig['pathway']\n",
    "\n",
    "        loads=xr.concat( (loads,watershed_loads), dim='source')\n",
    "\n",
    "        # drop 'stormwater' to catch potential bugs later\n",
    "        return loads.isel(source=(loads.source!='stormwater'))\n",
    "\n",
    "    @memoize.imemoize()\n",
    "    def bc_ds(self):\n",
    "        \"\"\" Extract the relevant parts of the BC data, return as a single dataset\n",
    "        \"\"\"\n",
    "        compiled_fn=os.path.join(self.sun_paths()[0],'bc_extracted_v4.nc')\n",
    "\n",
    "        if not os.path.exists(compiled_fn):\n",
    "            self.extract_bc_ds(compiled_fn)\n",
    "        ds=xr.open_dataset(compiled_fn)\n",
    "        return ds\n",
    "\n",
    "    def extract_bc_ds(self,compiled_fn):\n",
    "        dss=[]\n",
    "        dss_orig=[] # for closing\n",
    "\n",
    "        for run_path in self.sun_paths():\n",
    "            # only care about point sources, and river inflows (i.e. ignore\n",
    "            # ocean flux BCs, and any freesurface BCs\n",
    "            model=sun_driver.SuntansModel.load(run_path)\n",
    "            ds=model.load_bc_ds()\n",
    "            dss_orig.append(ds)\n",
    "            ti_start,ti_stop=np.searchsorted(ds.time.values,[model.run_start,model.run_stop])\n",
    "            ds=ds.isel(Nt=slice(ti_start,ti_stop+1))\n",
    "\n",
    "            for extra in ['T','S','h','boundary_h','boundary_w','boundary_T',\n",
    "                          'boundary_u','boundary_v','z',\n",
    "                          'point_S','point_T','cellp','xv','yv','uc','vc','wc']:\n",
    "                if extra in ds: del ds[extra]\n",
    "\n",
    "            type2_sel= ds['boundary_S'].isel(Nk=0,Nt=0)==0.0\n",
    "            ds=ds.isel(Ntype2=type2_sel)\n",
    "            del ds['boundary_S']\n",
    "            dss.append(ds)\n",
    "\n",
    "        trim_dss=[]\n",
    "        for ds in dss:\n",
    "            if not trim_dss:\n",
    "                trim_dss.append(ds)\n",
    "                continue\n",
    "            else:\n",
    "                t_sel=ds.time.values>trim_dss[-1].time.values[-1]\n",
    "                if t_sel.sum():\n",
    "                    trim_dss.append(ds.isel(Nt=t_sel))\n",
    "                else:\n",
    "                    log.warning(\"BC dataset had no useful times?\")\n",
    "\n",
    "        assert len(trim_dss)\n",
    "        ds=xr.concat(trim_dss,dim='Nt',data_vars='different')\n",
    "        # somehow there is some 1e-9 difference between xe \n",
    "        for v in ['xe','ye']:\n",
    "            if 'Nt' not in ds[v].dims: continue\n",
    "            # make sure it's not a terrible issue\n",
    "            assert ds[v].std(dim='Nt').max()<1.0\n",
    "            ds[v]=ds[v].isel(Nt=0)\n",
    "\n",
    "        # A bit goofy, but try to avoid a race condition, short of\n",
    "        # utilizing a real lock.\n",
    "        tmp_fn=compiled_fn+\".\"+str(os.getpid())\n",
    "        ds.to_netcdf(tmp_fn)\n",
    "        if not os.path.exists(compiled_fn):\n",
    "            os.rename(tmp_fn,compiled_fn)\n",
    "        else:\n",
    "            log.warning(\"Things got racy while transcribing BC data\")\n",
    "            os.unlink(tmp_fn)\n",
    "\n",
    "        [ds.close() for ds in dss_orig]\n",
    "\n",
    "    @memoize.imemoize()\n",
    "    def source_Qdata(self,source):\n",
    "        \"\"\"\n",
    "        returns a time series DataArray for the\n",
    "        respective source's flow rate.\n",
    "        This step also clamps flows to be non-negative.\n",
    "        \"\"\"\n",
    "        bc_ds=self.bc_ds()\n",
    "\n",
    "        # get a flow time series for this specific group\n",
    "        try:\n",
    "            seg_i=list(bc_ds.seg_name.values).index(source)\n",
    "            Q_time_series=bc_ds.set_coords('time')['boundary_Q'].isel(Nseg=seg_i)\n",
    "        except ValueError:\n",
    "            # point sources are labeled as srcNNN\n",
    "            pnt_i=int(source.replace('src',''))\n",
    "            Q_time_series=bc_ds.set_coords('time')['point_Q'].isel(Npoint=pnt_i)\n",
    "            \n",
    "        return Q_time_series.clip(0.0)\n",
    "    \n",
    "    # Particle->grid mapping code\n",
    "    def particles_to_conc(self,particles,\n",
    "                          count_field='mp_count',\n",
    "                          scale=1.0,smooth=0):\n",
    "        \"\"\"\n",
    "        sum particles counts (weights) per cell. Any aging or scaling\n",
    "        should be done by the caller, and point to the resulting particle\n",
    "        field in 'count_field'\n",
    "        Note that this just sums the counts, applying an optional scaling.\n",
    "        When particles covers multiple time steps of output, the caller\n",
    "        should probably provide a suitable scaling to average in time.\n",
    "        \n",
    "        Returns xr.Dataset\n",
    "        \"\"\"\n",
    "        grid=self.grid()\n",
    "        \n",
    "        mp_counts=particles.groupby('cell')[count_field].sum()\n",
    "\n",
    "        cell_counts=np.zeros(grid.Ncells(),np.float64)\n",
    "        cell_counts[mp_counts.index.values] = mp_counts.values\n",
    "        cell_conc=cell_counts/grid.cells_area()\n",
    "        for _ in range(smooth):\n",
    "            cell_conc=self.smoothM().dot(cell_conc)\n",
    "\n",
    "        cell_conc*=scale \n",
    "\n",
    "        ds=xr.Dataset()\n",
    "        ds['conc']=('cell',),cell_conc\n",
    "        return ds\n",
    "    \n",
    "    @memoize.imemoize()\n",
    "    def smoothM(self):\n",
    "        return self.grid().smooth_matrix()\n",
    "        \n",
    "class Run:\n",
    "    experiment=None\n",
    "    path=None\n",
    "    def __init__(self,**kw):\n",
    "        utils.set_keywords(self,kw)\n",
    "    def group_names(self):\n",
    "        bin_out_fns=glob.glob(os.path.join(self.path,\"*_bin.out\"))\n",
    "        group_names=[os.path.basename(p).replace('_bin.out','') for p in bin_out_fns]\n",
    "        return group_names\n",
    "    # @memoize.imemoize()\n",
    "    def groups(self):\n",
    "        return [Group(run=self,name=name) for name in self.group_names()]\n",
    "    def satisfies(self,criteria):\n",
    "        if 't_min' in criteria and self.t_max()<criteria['t_min']:\n",
    "            return False\n",
    "        if 't_max' in criteria and self.t_min()>criteria['t_max']:\n",
    "            return False\n",
    "        return True\n",
    "    #@memoize.imemoize()\n",
    "    def ptm_config(self):\n",
    "        return ptm_config.PtmConfig.load(self.path)\n",
    "    def t_max(self):\n",
    "        \"Maximum time of particle outputs for the run.\"\n",
    "        return self.ptm_config().end_time\n",
    "    def t_min(self):\n",
    "        \"Minimum time of particle outputs.\"\n",
    "        # relies on knowing that all groups follow the same start/end \n",
    "        # within a run\n",
    "        return self.groups()[0].t_min()\n",
    "    def query_particles(self,criteria):\n",
    "        results=[g.query_particles(criteria)\n",
    "                 for g in utils.progress(self.groups(),msg=\"  Groups %s\")\n",
    "                 if g.satisfies(criteria)]\n",
    "        if results:\n",
    "            df=pd.concat(results)\n",
    "            df['run']=self.path\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def query_groups(self,criteria):\n",
    "        results=[g for g in self.groups()\n",
    "                 if g.satisfies(criteria)]\n",
    "        return results\n",
    "\n",
    "# Try non-instance memoize to manage cache better.\n",
    "#@memoize.memoize(lru=20)\n",
    "def ptm_bin(fn):\n",
    "    return ptm_tools.PtmBin(fn=fn)\n",
    "\n",
    "class Group:\n",
    "    run=None\n",
    "    name=None\n",
    "    def __init__(self,**kw):\n",
    "        utils.set_keywords(self,kw)\n",
    "        m=re.match(r'(.*)_((up|down|none)\\d*)_rel(\\d+)',self.name)\n",
    "        self.source,self.behavior,self.release_date=m.group(1),m.group(2),m.group(4)\n",
    "    def satisfies(self,criteria):\n",
    "        if 'sources' in criteria and self.source not in criteria.sources:\n",
    "            return False\n",
    "        if 't_min' in criteria and self.t_max() < criteria['t_min']:\n",
    "            return False\n",
    "        if 't_max' in criteria and self.t_min() > criteria['t_max']:\n",
    "            return False\n",
    "        if 'behaviors' in criteria and self.behavior not in criteria['behaviors']:\n",
    "            return False\n",
    "        return True\n",
    "    @memoize.imemoize()\n",
    "    def t_max(self):\n",
    "        return self.run.t_max()\n",
    "    @memoize.imemoize()\n",
    "    def t_min(self):\n",
    "        release_log_fn=os.path.join(self.run.path,self.name+\".release_log\")\n",
    "        with open(release_log_fn,'rt') as fp:\n",
    "            rel_date,rel_time=fp.readline().strip().split()[-2:]\n",
    "        return np.datetime64(rel_date+\" \"+rel_time)\n",
    "    def ptm_bin(self):\n",
    "        return ptm_bin(os.path.join(self.run.path,self.name+\"_bin.out\"))\n",
    "    @memoize.imemoize()\n",
    "    def release_log(self):\n",
    "        fn=os.path.join(self.run.path,self.name+\".release_log\")\n",
    "        # df_rel=ptm_tools.release_log_dataframe(fn=fn)\n",
    "        rel=ptm_tools.ReleaseLog(fn)\n",
    "        df_rel=rel.data\n",
    "        # release has id and gid. So far they are always the same\n",
    "        # Trust, but verify.\n",
    "        assert np.all(df_rel['id'].values==df_rel['gid'].values),\"Invariant check\"\n",
    "        return rel\n",
    "    \n",
    "    def query_particles(self,criteria):\n",
    "        pbf=self.ptm_bin()\n",
    "        times=np.array( [utils.to_dt64(t) for t in pbf.time] )\n",
    "        sel=np.ones(len(times),np.bool8)\n",
    "        if 't_min' in criteria:\n",
    "            sel=sel&(times>=criteria['t_min'])\n",
    "        if 't_max' in criteria:\n",
    "            sel=sel&(times<=criteria['t_max'])\n",
    "\n",
    "        all_particles=[]\n",
    "        for ts in np.nonzero(sel)[0]:\n",
    "            datenum,particles=pbf.read_timestep(ts)\n",
    "            t=utils.to_dt64(datenum)\n",
    "            #particles=self.filter_particles(particle,criteria)\n",
    "            #df=pd.DataFrame(particles) # Can't deal with vectors\n",
    "            df=pd.DataFrame()\n",
    "            df['id']=particles['id']\n",
    "            df['x0']=particles['x'][:,0]\n",
    "            df['x1']=particles['x'][:,1]\n",
    "            df['x2']=particles['x'][:,2]\n",
    "            df['active']=particles['active']\n",
    "            # Avoid keeping all this memmap'd data around\n",
    "            del particles\n",
    "            df['time']=t\n",
    "            all_particles.append(df)\n",
    "        if len(all_particles):\n",
    "            df=pd.concat(all_particles)\n",
    "            df['group']=self.name\n",
    "            \n",
    "            # Assign release times\n",
    "            # If this is slow, could verify that release_log index \n",
    "            # is dense, and do this in numpy\n",
    "            df_rel=self.release_log().data.set_index('id') # id vs gid?\n",
    "            rel_times=df_rel.loc[ df['id'].values,'date_time']\n",
    "            df['rel_time']=rel_times.values\n",
    "            \n",
    "            if 'age_max' in criteria:\n",
    "                sel = (df['time']-df['rel_time']) < criteria['age_max']\n",
    "                if np.any(sel):\n",
    "                    df=df[sel].copy() # otherwise further operations will error\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "                \n",
    "            self.add_mp_count(df,criteria)\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    def w_s(self):\n",
    "        w_s_map={'down50000':0.05,\n",
    "                 'down5000':0.005,\n",
    "                 'down500':0.0005,\n",
    "                 'none':0.0,\n",
    "                 'up500':0.0005,\n",
    "                 'up5000':0.005,\n",
    "                 'up50000':0.05}\n",
    "        return w_s_map[self.behavior]\n",
    "    \n",
    "    # For a subset of the source names in the PTM setup,\n",
    "    # map them to the name used in the loading data\n",
    "    source_ptm_to_load={\n",
    "        'cccsd':'CCCSD',\n",
    "        'sunnyvale':'SUNN',\n",
    "        'fs':'FSSD',\n",
    "        'palo_alto':'PA',\n",
    "        'san_jose':'SJ',\n",
    "        'src000':'EBDA',\n",
    "        'src001':'EBMUD',\n",
    "        'src002':'SFPUC',\n",
    "        # These shouldn't be used, but including just to be sure\n",
    "        # that if they somehow show up, they won't contaminate\n",
    "        # stormwater.\n",
    "        'SacRiver':'DELTA',\n",
    "        'SJRiver':'DELTA'\n",
    "    }\n",
    "\n",
    "    def load_conc(self,categories='all'):\n",
    "        \"\"\"\n",
    "        Concentration for this particle group, in particles/liter.\n",
    "        categories: 'all','nonfiber', a list of category names,\n",
    "        or a single category name\n",
    "        \"\"\"\n",
    "        load_data=self.run.experiment.load_data()\n",
    "        # source: e.g. 'San_Lorenzo_C'\n",
    "        if self.source in self.source_ptm_to_load:\n",
    "            load_source=self.source_ptm_to_load[self.source]\n",
    "        else:\n",
    "            load_source=self.source\n",
    "            \n",
    "        conc_over_category=load_data['conc'].sel(source=load_source,w_s=self.w_s())\n",
    "        if categories=='all':\n",
    "            conc=conc_over_category.values.sum()\n",
    "        elif categories=='nonfiber':\n",
    "            sel=conc_over_category.category!='Fiber'\n",
    "            conc=conc_over_category.isel(category=sel).values.sum()\n",
    "        else:\n",
    "            conc=conc_over_category.sel(category=categories).values.sum()\n",
    "        return conc\n",
    "    \n",
    "    def Qdata(self):\n",
    "        return self.run.experiment.source_Qdata(self.source)\n",
    "\n",
    "    def volume_per_particle(self,particles):\n",
    "        \"\"\"\n",
    "        Need particles/per time for release time t\n",
    "        and volume/time for source at time t.\n",
    "        Returns array with volume per particle for each of the given particles.\n",
    "\n",
    "        Some parts of this aren't really group specific, could be moved elsewhere.\n",
    "        \"\"\"\n",
    "        # Sanity check:\n",
    "        # this is giving about 60m3/particle.\n",
    "        # I know there are 10 particles/hr released.\n",
    "        # so that's saying that San_Lorenzo_C[reek] has\n",
    "        # a flow of 600m3/hour, or 0.17 m3/s in June 2017.\n",
    "        # USGS measured is 6-8 cfs, which is order 0.2 m3/s.\n",
    "        # Good.\n",
    "\n",
    "        # pbf=run.open_binfile(group)\n",
    "        rel=self.release_log()\n",
    "\n",
    "        # Record the data from release_log\n",
    "        release=rel.intervals\n",
    "        # release['group_id']=group_id\n",
    "        # release['epoch']=(release['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "        # Ham-handed fix up of truncated release logs.\n",
    "        typical_count=int( np.median( release['count'] ) )\n",
    "        bad=release['count']!=typical_count\n",
    "        # a little bit smart -- only step in when it's the last step that's\n",
    "        # different.\n",
    "        if (not np.any(bad[:-1])) and bad[-1]:\n",
    "            print(\"Yikes - release_log might be missing some particles.\")\n",
    "            print(\"  Changing reported count of %d to %d\"%(release['count'].values[-1],\n",
    "                                                           typical_count))\n",
    "            print(\"  And punting on id_max,gid_max\")\n",
    "            release.loc[bad,'count']=typical_count\n",
    "            # careful of inclusive indexes\n",
    "            release.loc[bad,'id_max']=release.loc[bad,'id_min']+typical_count-1\n",
    "            release.loc[bad,'gid_max']=release.loc[bad,'gid_min']+typical_count-1\n",
    "\n",
    "        # add in volume information.\n",
    "        Qdata=self.Qdata() \n",
    "        Qvalues=Qdata.values\n",
    "        Qpart=np.zeros(len(particles),np.float64)\n",
    "        Qpart[:]=np.nan # for safety\n",
    "        # This could be sped up since the same time will appear many times\n",
    "        # in particles. Does it matter?\n",
    "        for t,part_idxs in utils.enumerate_groups(particles['rel_time'].values):\n",
    "            Qidx=np.searchsorted(Qdata.time.values,t)\n",
    "            Qpart[part_idxs]=Qvalues[Qidx]\n",
    "\n",
    "        grp_hour_per_rel=(release['time'].values[1] - release['time'].values[0])/np.timedelta64(3600,'s')\n",
    "\n",
    "        # m3/s * s/hour * hour/release / (particles/release) => m3/particle\n",
    "        part_volume=Qpart*3600 * grp_hour_per_rel/typical_count\n",
    "        return part_volume # m3/particle\n",
    "    def add_mp_count(self,particles,criteria):\n",
    "        category=criteria.get('category','all')\n",
    "        mp_per_liter=self.load_conc(category)\n",
    "\n",
    "        # how many m3 of effluent does each computational particle\n",
    "        # represent?\n",
    "        m3_per_particle=self.volume_per_particle(particles)\n",
    "        #                  mp_count / m3   *   m3 / particle\n",
    "        mp_per_particle=mp_per_liter * 1e3 * m3_per_particle\n",
    "        particles['mp_count']=mp_per_particle\n",
    "        \n",
    "\n",
    "experiment=Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So when querying something like 15 days, I get memory issues.\n",
    "# currently things are divied up by run, and a single run might\n",
    "# have 90+ groups.\n",
    "# Would be better to divy by group.\n",
    "\n",
    "# What does run do right now?\n",
    "# \n",
    "\n",
    "def age_particles(particles,tau_s,target='aged'):\n",
    "    age_s=(particles['time']-particles['rel_time'])/np.timedelta64(1,'s')\n",
    "    particles[target]=particles['mp_count']*np.exp(-age_s/tau_s)\n",
    "    return particles\n",
    "\n",
    "@utils.add_to(experiment)\n",
    "def query_particle_concentration(self,criteria,decay=None):\n",
    "    \"\"\"\n",
    "    Map particles, with optional decay function, onto grid \n",
    "    as concentrations. Returns xr.Dataset suitable for BayConcFigure.\n",
    "    \"\"\"\n",
    "    # First, take a sequential approach, time it, then come back\n",
    "    # to push more postprocessing out to nodes.\n",
    "    if 1: \n",
    "        # For long-ish time periods this runs into serious memory issues.\n",
    "        print(\"Query particles\")\n",
    "        parts=self.query_particles(criteria=criteria)\n",
    "        print(\"Computing...\")\n",
    "        particles=parts.compute()\n",
    "        print(f\"Compute done. {len(particles)} particles\")\n",
    "    else:\n",
    "        HERE\n",
    "        # HERE -- need to fold together the steps for putting it on a grid\n",
    "        # and the query_particles work, so that the amount of data coming back\n",
    "        # from each bag is manageable.\n",
    "        def query_particles(self,criteria):\n",
    "            \"\"\"\n",
    "            criteria: t_min, t_max ~ np.datetime64\n",
    "              behaviors: list of strings, 'down5000','none',etc.\n",
    "              sources: list of strings, shortened names.\n",
    "            returns dask dataframe.\n",
    "            \"\"\"\n",
    "            # combine the query and any processing that can still be\n",
    "            # done on individual tables of particles.\n",
    "            run_bag=self.runs_bag(criteria=criteria)\n",
    "\n",
    "            def query_process(run):\n",
    "                particles=run.query_particles(criteria)\n",
    "                self.add_particle_attrs(particles,inplace=True)\n",
    "                particles_vert=self.select_by_vertical(particles,criteria)\n",
    "                return particles_vert\n",
    "\n",
    "            part_bag=run_bag.map(query_process)\n",
    "            part_dds=part_bag.map(dd.from_pandas,npartitions=1)\n",
    "            part_dds_comp=part_dds.compute()\n",
    "            part_vert_sel=dd.concat(part_dds_comp)\n",
    "\n",
    "            return part_vert_sel\n",
    "\n",
    "    age_s=(particles['time']-particles['rel_time'])/np.timedelta64(1,'s')\n",
    "    age_max=criteria.get('age_max', np.timedelta64(60,'D'))\n",
    "    age_max_s=age_max/np.timedelta64(1,'s')\n",
    "    weights=np.where( age_s<age_max_s,1.0,0.0)\n",
    "    \n",
    "    if decay is not None:\n",
    "        decay_s=decay/np.timedelta64(1,'s')\n",
    "        weights*=np.exp(-age_s/tau_s)\n",
    "        \n",
    "    particles['weighted_count']=weights*particles['mp_count'].values\n",
    "    \n",
    "    output_steps=1+int( (criteria['t_max'] - criteria['t_min'])/self.ptm_output_interval)\n",
    "    scale=1./output_steps\n",
    "    \n",
    "    ds=self.particles_to_conc(particles,smooth=0,scale=scale,count_field='weighted_count')\n",
    "\n",
    "    ds['conc'].attrs['grp_filter']=criteria.get('behavior','')\n",
    "    ds['criteria']=(),criteria\n",
    "    \n",
    "    # Manual determination of name of z_filter\n",
    "    z_filters=[]\n",
    "    if 'z_below_surface_max' in criteria:\n",
    "        z_filters.append(\"surface %.3f\"%criteria['z_below_surface_max'])\n",
    "    if 'z_above_bed_max' in criteria:\n",
    "        z_filters.append(\"bed %.3f\"%criteria['z_above_bed_max'])\n",
    "    \n",
    "    ds['conc'].attrs['z_filter']=\" and \".join(z_filters)\n",
    "    ds['conc'].attrs['t_start']=criteria['t_min']\n",
    "    ds['conc'].attrs['t_stop']=criteria['t_max']\n",
    "    ds['conc'].attrs['max_age']=age_max\n",
    "    ds['conc'].attrs['scale']=scale\n",
    "    ds['conc'].attrs['output_steps']=output_steps\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query particles\n",
      "Narrowed to 8 runs\n",
      "Computing...\n",
      "Compute done. 2686013 particles\n",
      "CPU times: user 22.5 s, sys: 5.03 s, total: 27.5 s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This is now taking 15 minutes.\n",
    "# probably because of the larger number of groups that are active\n",
    "# at the end of Aug.\n",
    "criteria=dict(t_min=np.datetime64(\"2017-06-30 00:00\"), # should be 08/30\n",
    "              t_max=np.datetime64(\"2017-07-01 00:00\"), # should be 09/14\n",
    "              category='nonfiber',\n",
    "              z_below_surface_max=0.095,\n",
    "              age_max=np.timedelta64(10,'D'))\n",
    "\n",
    "conc_ds=experiment.query_particle_concentration(criteria=criteria,\n",
    "                                                decay=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing some of the memoizing which seems to have caused\n",
    "# some locking issues, it completes but is kind of on the slow side.\n",
    "# Test the effect of the repartition:\n",
    "#  group bag repartitions into 50*run_bag.npartitions\n",
    "#  400 tasks query tasks, and from_pandas. => 3m18.\n",
    "\n",
    "#  group bag repartitions into 20*run_bag_npartitions\n",
    "#  Still reports 400 tasks. 3m20s.\n",
    "\n",
    "#  Compare-ish to nonsql, non-dask code.  Not a good comparison\n",
    "#  because that code doesn't check age_max.\n",
    "\n",
    "#  And single core is still slightly faster. Well, after another shot,\n",
    "# dask came in at 2m40. but that's 8 cores!\n",
    "# 22% of the dask time is reading the grid.  Try pre-loading the grid.\n",
    "# Now it reports 160 tasks.  Wondering if the partition change before\n",
    "# did not take effect.\n",
    "# Moving the grid reading out, profile no longer shows the grid reading\n",
    "# as significant. Time is down to 2m16. 34% of the time, according to \n",
    "# profile view, we're creating Paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria=dict(t_min=np.datetime64(\"2017-08-30 00:00\"),\n",
    "              t_max=np.datetime64(\"2017-09-14 00:00\"),\n",
    "              category='nonfiber',\n",
    "              z_below_surface_max=0.095,\n",
    "              age_max=np.timedelta64(10,'D'))\n",
    "\n",
    "# parts=experiment.query_particles(criteria=criteria)\n",
    "conc=experiment.query_particle_concentration(criteria=criteria)\n",
    "\n",
    "print_rusage() # 2GB here\n",
    "\n",
    "particles=parts.compute()\n",
    "\n",
    "print(f\"{len(particles)} particles found\")\n",
    "print_rusage() # 3.2GB here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps\n",
    "--\n",
    "\n",
    "The new organization, with repartitioning and more per-group structure,\n",
    "seems promising speed-wise but it never terminates.\n",
    "\n",
    "Is it related to too much caching/memoizing?  Trying to drop most of that\n",
    "from Run. That does appear to make the scheduling much better -- near\n",
    "full utilization, where it had been sort of scattered.\n",
    "\n",
    "1. Recreate some of the figures from before, including on-grid smoothing.  \n",
    "  a. Sample plot: from the powerpoint. 2017-08-30 to 2017-09-14. Surface particles\n",
    "     max age of 10 days.   \n",
    "2. Pull out manta samples as before. Maybe skip putting it on the grid, just\n",
    "   query a radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conc_figure\n",
    "six.moves.reload_module(conc_figure)\n",
    "from matplotlib import cm\n",
    "cmap=cm.CMRmap_r\n",
    "cmap=scmap.cmap_clip(cmap,0.03,1.0)\n",
    "\n",
    "conc_figure.BayConcFigure(conc_ds,grid=experiment.grid(),cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I lose 2.5 to 7GB each time I run this multithreaded,\n",
    "# or 300MB single threaded.\n",
    "\n",
    "# With that mallopt setting, I lose about 1MB per invocation.\n",
    "\n",
    "import dask.array as da\n",
    "x = da.ones((2e4, 2e4), chunks=(2e4, 100))\n",
    "y = x.rechunk((100, 2e4))\n",
    "z = y.rechunk((2e4, 100))\n",
    "\n",
    "print_rusage()\n",
    "# 80MB\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "# z.sum().compute(scheduler='single-threaded')  # This doesn't cause problems\n",
    "z.sum().compute(scheduler='threads')  # This leaks around 500MB of memory\n",
    "\n",
    "print_rusage()\n",
    "# 500-600MB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
