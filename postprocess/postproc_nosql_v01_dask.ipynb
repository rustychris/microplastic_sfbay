{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Approach to Non-SQL PTM Queries\n",
    "--\n",
    "\n",
    "v01: move away from classes, try to organize the computation \n",
    "primarily in dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:postproc:Configuring malloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import postproc_dask as post\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.close()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475b076e80374ef6ad21ccdb94c773ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\n            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-Ouâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing.popen_spawn_posix #  https://github.com/dask/distributed/issues/4168\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "\n",
    "from dask.distributed import Client\n",
    "client=Client(n_workers=8,threads_per_worker=1)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import conc_figure\n",
    "import six\n",
    "import stompy.plot.cmap as scmap\n",
    "from stompy.spatial import proj_utils\n",
    "from matplotlib import cm\n",
    "cmap=cm.CMRmap_r\n",
    "cmap=scmap.cmap_clip(cmap,0.03,1.0)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import re\n",
    "\n",
    "from stompy.grid import unstructured_grid\n",
    "from stompy import utils, memoize\n",
    "from stompy.model.fish_ptm import ptm_config, ptm_tools\n",
    "from stompy.model.suntans import sun_driver\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import stompy.plot.cmap as scmap\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "turbo=scmap.load_gradient('turbo.cpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Process\n",
    "===\n",
    "\n",
    "1. SUNTANS hydro runs\n",
    "2. SUNTANS average output\n",
    "3. ptm-formatted average output\n",
    "4. PTM runs\n",
    "5. Load data\n",
    "\n",
    "The top-level query is something like *generate a map of concentrations for...*\n",
    "\n",
    "filter on:\n",
    " - sources $x$\n",
    " - settling classes $y$\n",
    " - vertical positions $z$\n",
    " - horizontal positions $h$\n",
    "\n",
    "weighted by\n",
    "\n",
    " - loading data \n",
    " - age\n",
    " \n",
    "mapped by one of ...\n",
    "\n",
    " - bounding box\n",
    " - put on hydro grid\n",
    " - put on regular grid\n",
    "\n",
    "and possibly smoothed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment level configuration -- small, all python native data.\n",
    "import cfg_v01 \n",
    "cfg=dict(cfg_v01.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grid into... grid\n",
    "hydro_path=cfg['sun_paths'][0]\n",
    "ptm_ds=xr.open_dataset(os.path.join(hydro_path,\"ptm_average.nc_0000.nc\"))\n",
    "grid=unstructured_grid.UnstructuredGrid.read_ugrid(ptm_ds,dialect='fishptm')\n",
    "ptm_ds.close()   \n",
    "\n",
    "# distribute to workers ahead of time.\n",
    "grid_d=client.scatter(grid)\n",
    "cfg['grid_d']=grid_d # too far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:65784/99089\n",
      "INFO:utils:65475/99089\n"
     ]
    }
   ],
   "source": [
    "# So far this is only used locally.  Slow to compute (15s)\n",
    "Msmooth=grid.smooth_matrix()\n",
    "Msmooth_K=grid.smooth_matrix(K=100*np.ones(grid.Nedges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or could make this delayed and have it execute on each client?\n",
    "load_data_d=client.scatter(post.get_load_data())\n",
    "cfg['load_data_d']=load_data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydro timestamps\n",
    "cfg['hydro_timestamps']=post.load_hydro_timestamps(cfg['sun_paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_ds_d=client.scatter(post.bc_ds(cfg=cfg))\n",
    "cfg['bc_ds_d']=bc_ds_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas=grid.cells_area()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this a bit early, as it controls cache location which \n",
    "# is needed for some testing.\n",
    "cfg['manta_out_dir']=\"manta_sets_20210726d\"\n",
    "\n",
    "# now the 021b runs.\n",
    "# c suffix: use tighter -0.095 z filter.\n",
    "# 20200227: start of v03\n",
    "# 20200302: start of v04\n",
    "# 20200305: try larger window -- extra 24 hours in both directions.\n",
    "# 20200312: new sources in ptm data, with v02 stormwater concs.\n",
    "# 20200330: more new sources.\n",
    "# 20210210a: dask code, new hydro, new ptm. new year.\n",
    "# 20210317a: new ptm runs with no SJ loss...\n",
    "#            has the w_s bug\n",
    "# 20210317b: fixed w_s bug\n",
    "# 20210317c: switch to weathered loads\n",
    "# 20210708c: including beaching and deposition counts.\n",
    "os.makedirs(cfg['manta_out_dir'],exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaching and Deposition\n",
    "---\n",
    "\n",
    "Beaching and deposition seem to be more likely candidates for scale offsets \n",
    "than sinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:postproc:Configuring malloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Testing query_group_particles:\n",
    "six.moves.reload_module(post)\n",
    "if 0:\n",
    "    run=cfg['ptm_run_paths'][0]\n",
    "    grps=post.run_to_group_paths(run)\n",
    "    grp=grps[0]\n",
    "    load_data=post.get_load_data()\n",
    "    bc_ds=post.bc_ds(cfg=cfg)\n",
    "\n",
    "    df=post.query_group_particles(grp,\n",
    "                               dict(t_min=np.datetime64('2017-06-30'),\n",
    "                                    t_max=np.datetime64('2017-07-04'),\n",
    "                                    age_max=np.timedelta64(60,'D')),\n",
    "                               load_data=load_data,\n",
    "                               bc_ds=bc_ds)\n",
    "\n",
    "    df2=post.query_group_particles(None,\n",
    "                                   dict(t_min=np.datetime64('2017-06-30'),\n",
    "                                        t_max=np.datetime64('2017-07-04'),\n",
    "                                        age_max=np.timedelta64(60,'D')),\n",
    "                                   load_data=load_data,\n",
    "                                   bc_ds=bc_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 8.78 s, total: 20.8 s\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "%time df=post.particles_for_date(\"2017-10-21 00:00:00\",cfg=cfg,cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41522146"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) # 41522151, took 39 minutes with 8 processes, warm-ish.\n",
    "# 22 minutes warm, on 24 processes.\n",
    "# Compare that to 17.4 minutes via mp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.433333333333334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1046/60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large is that result?\n",
    "# Using just the first 50 groups, get 431557 rows.\n",
    "# \n",
    "# 11 fields are 8 bytes\n",
    "# 3 4 bytes\n",
    "# 2 2 bytes\n",
    "# one string\n",
    "# \n",
    "rec_size=11*8 + 3*4 + 2*2 + 1*8\n",
    "rec_size # 112 bytes per\n",
    "rec_size*431557 # 48M\n",
    "rec_size*41522151 / 1e9\n",
    "# So a final result should be about 5G\n",
    "# postproc_mp was using 10x more than that in a single process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a particle, and count its encounters with the bed and the shore.\n",
    "grp # Alameda down50000\n",
    "\n",
    "info=xr.open_dataset(grp+\"-v00.nc\")\n",
    "\n",
    "pid=10\n",
    "\n",
    "particles=post.query_group_particles(grp,\n",
    "                           dict(#t_min=np.datetime64('2017-06-30'),\n",
    "                                #t_max=np.datetime64('2017-07-04'),\n",
    "                                age_max=np.timedelta64(60,'D')),\n",
    "                           load_data=load_data,\n",
    "                           bc_ds=bc_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# len(particles) # 3M\n",
    "particle=particles[ particles['id']==10 ]\n",
    "len(particle)\n",
    "\n",
    "z_bed=grid.cells['z_bed'][particle['cell']]\n",
    "zab=particle['x2']-z_bed\n",
    "\n",
    "plt.figure()\n",
    "grid.plot_edges(lw=0.5,color='0.6')\n",
    "plt.plot(particle['x0'],particle['x1'],'k-',zorder=0)\n",
    "scat=plt.scatter(particle['x0'],particle['x1'],30,zab,cmap='turbo',zorder=1)\n",
    "scat.set_clim([-0.1,0.5])\n",
    "plt.colorbar(scat)\n",
    "plt.axis('tight')\n",
    "plt.axis('equal')\n",
    "\n",
    "# zab goes down to 2e-5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(zab,bins=np.linspace(0,1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zab.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps\n",
    "--\n",
    "\n",
    "\n",
    "1. Recreate some of the figures from before, including on-grid smoothing.  \n",
    "  a. Sample plot: from the powerpoint. 2017-08-30 to 2017-09-14. Surface particles\n",
    "     max age of 10 days.   *This plot is similarish -- not exactly the same but close\n",
    "     enough to rule out fundamental errors*\n",
    "2. Pull out manta samples as before. Maybe skip putting it on the grid, just\n",
    "   query a radius.\n",
    "  a. Implement in this notebook\n",
    "  b. Move all of this to a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll2utm=proj_utils.mapper('WGS84','EPSG:26910')\n",
    "# Load the manta data, clean it up\n",
    "manta_fn='manta_summary-v03.csv'\n",
    "manta=pd.read_csv(manta_fn)\n",
    "manta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "godwin_offset_h=post.godwin_offset_h\n",
    "godwin=post.godwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cfg['ptm_run_paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mantas_multi(manta,taus,\n",
    "                         Msmooths=[Msmooth],\n",
    "                         steps=[30],\n",
    "                         storm_factors=[0.05],\n",
    "                         cfg=cfg):\n",
    "    # manta: pd.DataFrame \n",
    "    # tau: np.timedelta64 giving e-folding time scale\n",
    "    # of loss.\n",
    "    # storm_factor: scale stormwater concentrations by this\n",
    "    #  factor.\n",
    "    preds=[]\n",
    "    \n",
    "    # Force mantas to be sorted, so that data from the same day\n",
    "    # is grouped together.\n",
    "    manta=manta.sort_values('DATE')\n",
    "    last_DATE=None\n",
    "    df=None\n",
    "\n",
    "    for idx,rec in utils.progress(manta.iterrows()):\n",
    "        print(idx)\n",
    "        # since we're just pulling out whole days, cache by day.\n",
    "        # and pull through the end of that day\n",
    "        # 5s, all in read_parquet\n",
    "        if rec.DATE==last_DATE:\n",
    "            pass\n",
    "        else:\n",
    "            df=post.particles_for_date(rec.DATE,cfg=cfg)\n",
    "            last_DATE=rec.DATE\n",
    "\n",
    "        # Trawl-specific processing:\n",
    "        # Pull subset of cells\n",
    "        # add a cell weighting, and a time weighting for the tidal filter\n",
    "        for Msmooth_i,Msmooth in enumerate(Msmooths):\n",
    "            print(f\"  smooth: {Msmooth_i}\")\n",
    "            for step in steps:\n",
    "                print(f\"    steps: {step}\")\n",
    "                cells,weights=post.rec_to_cell_weights(rec,areas=areas,grid=grid,\n",
    "                                                       Msmooth=Msmooth,smooth=step,thresh=1e-5)\n",
    "                df_spatial=pd.DataFrame(dict(cell=cells,weight_spatial=weights)).set_index('cell')\n",
    "                # 1.3s. selecting a subset of rows, and adding a column with spatial weight.\n",
    "                # Using a dictionary might get this down to 60% of original\n",
    "                # going through numpy  might get down to 40% of original.\n",
    "                if 0:\n",
    "                    tdf=df.join(df_spatial,on='cell',how='inner')\n",
    "                else:\n",
    "                    # trying numpy approach\n",
    "                    df_spatial_sort=df_spatial.sort_index()\n",
    "                    space_cell=df_spatial_sort.index.values\n",
    "                    space_weight=df_spatial_sort['weight_spatial'].values\n",
    "                    df_cell=df.cell.values\n",
    "                    df_idx=np.searchsorted(space_cell,df_cell).clip(0,len(space_cell)-1)\n",
    "                    sel=space_cell[df_idx]==df_cell\n",
    "                    tdf=df[sel].copy()\n",
    "                    tdf['weight_spatial']=space_weight[df_idx[sel]]                    \n",
    "\n",
    "                t_sample=np.datetime64(rec['DATE'])    \n",
    "                t_center = t_sample+np.timedelta64(8,'h') + np.timedelta64(12,'h')\n",
    "                delta_hours=((tdf['time']-t_center)/np.timedelta64(1,'h')).astype(np.int32)\n",
    "                weight_time=godwin[delta_hours+godwin_offset_h]\n",
    "                age=tdf['time'] - tdf['rel_time']\n",
    "    \n",
    "                for storm_factor in storm_factors:\n",
    "                    print(f\"      storm_factor: {storm_factor}\")\n",
    "                    group_weight=post.group_weights(tdf,storm_factor)\n",
    "\n",
    "                    for tau in taus:\n",
    "                        tau_s=tau/np.timedelta64(86400,'s')\n",
    "                        print(f\"        tau={tau_s} days\")\n",
    "                        pred=dict(idx=idx,storm_factor=storm_factor,tau=tau,step=step,\n",
    "                                  Msmooth_i=Msmooth_i)\n",
    "                        pred['idx']=idx\n",
    "\n",
    "                        if tau_s>0:\n",
    "                            decay=np.exp( -age/tau )\n",
    "                        else:\n",
    "                            decay=1.0\n",
    "                        weights=tdf['weight_spatial']*weight_time*decay*group_weight\n",
    "\n",
    "                        # tdf['mp_per_particle'] # the number of physical particles represented by the virtual particle\n",
    "                        #   This is the contribution from this one\n",
    "                        #   virtual particle the cell concentration\n",
    "                        C=( weights * tdf['mp_per_particle'] / areas[tdf['cell']] ).sum()\n",
    "                        pred['C_model']=C\n",
    "                        preds.append(pred)\n",
    "\n",
    "    df_pred=pd.DataFrame(preds)\n",
    "    manta_pred=df_pred.join(manta,on='idx')\n",
    "    return manta_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(grp):\n",
    "    obs=grp['part_per_m2_nofiber'].values\n",
    "    mod=grp['C_model'].values\n",
    "    \n",
    "    skill={}\n",
    "    skill['rho'],skill['rho_p']=spearmanr(obs, mod)\n",
    "    skill['norm_std']=mod.std() / obs.std()\n",
    "    skill['norm_mag']=mod.mean() / obs.mean()\n",
    "    skill['rel_std_obs']=obs.std() / obs.mean()\n",
    "    skill['rel_std_mod']=mod.std() / mod.mean()\n",
    "    \n",
    "    # log-transformed\n",
    "    min_nonzero=obs[obs>0].min()\n",
    "    r_log=np.corrcoef(np.log(obs.clip(min_nonzero)),\n",
    "                      np.log(mod.clip(min_nonzero)))[0,1]\n",
    "    skill['r_log']=r_log\n",
    "    \n",
    "    # Same but drop top/bottom 2 outliers:\n",
    "    obs_rank=np.argsort(np.argsort(obs))\n",
    "    sel=(obs_rank>2)&(obs_rank<len(obs)-2)\n",
    "    r=np.corrcoef(obs[sel], mod[sel])[0,1]\n",
    "    skill['r_sub']=r\n",
    "    \n",
    "    skill['norm_std_sub']=mod[sel].std() / obs[sel].std()\n",
    "    skill['norm_mag_sub']=mod[sel].mean() / obs[sel].mean()\n",
    "    skill['rel_std_obs_sub']=obs[sel].std() / obs[sel].mean()\n",
    "    skill['rel_std_mod_sub']=mod[sel].std() / mod[sel].mean()\n",
    "\n",
    "    return pd.Series(skill)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # Evaluate effect of smoothing on correlation metrics\n",
    "    # This one testing evaluation of multiple smooths, steps,\n",
    "    # tau, etc. in one call.\n",
    "    taus=[np.timedelta64(int(25*86400),'s')]\n",
    "    Msmooths=[Msmooth,Msmooth_K]\n",
    "    mtypes=['grid','constant']\n",
    "    steps=[0,-10,-100,-1000,-10000,-100000]\n",
    "\n",
    "    manta_pred=predict_mantas_multi(manta,taus=taus,\n",
    "                                    Msmooths=Msmooths,\n",
    "                                    steps=steps,\n",
    "                                    storm_factors=[1.0])\n",
    "    manta_pred['Msmooth_label']=[ mtypes[i] for i in manta_pred['Msmooth_i'].values]\n",
    "\n",
    "    manta_pred.groupby(['Msmooth_label','step','tau','storm_factor']).apply(metrics)\n",
    "\n",
    "    taus=[np.timedelta64(int(d*86400),'s') for d in [1,2,5,10,20,30,45,60]]\n",
    "    Msmooths=[Msmooth]\n",
    "    mtypes=['grid']\n",
    "    steps=[-10000]\n",
    "\n",
    "    manta_pred_tau=predict_mantas_multi(manta,taus=taus,\n",
    "                                        Msmooths=Msmooths,\n",
    "                                        steps=steps,\n",
    "                                        storm_factors=[1])\n",
    "    manta_pred_tau['Msmooth_label']=[ mtypes[i] for i in manta_pred_tau['Msmooth_i'].values]\n",
    "\n",
    "    manta_pred_tau.groupby(['Msmooth_label','step','tau','storm_factor']).apply(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master set of predictions\n",
    "taus=[np.timedelta64(int(d*86400),'s') for d in [5,10,20,30,60]]\n",
    "Msmooths=[Msmooth,Msmooth_K]\n",
    "mtypes=['grid','constant']\n",
    "steps=[0,-10,-1000,-10000,-100000]\n",
    "storm_factors=[0.,0.01,0.05,0.2,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "manta_pred_master=predict_mantas_multi(manta,taus=taus,\n",
    "                                       Msmooths=Msmooths,\n",
    "                                       steps=steps,\n",
    "                                       storm_factors=storm_factors)\n",
    "manta_pred_master['Msmooth_label']=[ mtypes[i] for i in manta_pred_master['Msmooth_i'].values]\n",
    "\n",
    "manta_pred_master.groupby(['Msmooth_label','step','tau','storm_factor']).apply(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manta_pred_master['tau']=manta_pred_master['tau']/np.timedelta64(86400,'s')\n",
    "manta_pred_master.to_parquet('manta_pred_master.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics=manta_pred_master.groupby(['Msmooth_label','step','tau','storm_factor']).apply(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best overall rho:\n",
    "#  tau=5, storm=1, grid smooth, step=-100000 => rho=0.74\n",
    "#  insensitive to tau. mag still something like 14x too large for tau=60,\n",
    "#  interestingly, even with tau=5, magnitude still 7x too large.\n",
    "all_metrics.sort_values('rho',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so an r_log of 0.717 is r2\n",
    "all_metrics['r2_log']=all_metrics['r_log']**2\n",
    "all_metrics.sort_values('r_log',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best r_sub is also grid smoothed, step=-100000, but now\n",
    "# tau=60.0. Again, not very sensitive to tau or storm_factor.\n",
    "all_metrics.sort_values('r_sub',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best magnitudes, including obs. outliers?\n",
    "# This is all the place. Chooses some very noisy options,\n",
    "# probably just happenstance to get close.  All pretty low rho\n",
    "# and r_sub.\n",
    "all_metrics['norm_mag_err']=np.abs(all_metrics['norm_mag']-1)\n",
    "all_metrics.sort_values('norm_mag_err').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These mostly toss stormwater entirely.\n",
    "# Again, poor rho and r_sub values.\n",
    "all_metrics['norm_mag_sub_err']=np.abs(all_metrics['norm_mag_sub']-1)\n",
    "all_metrics.sort_values('norm_mag_sub_err').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(6,2.5))\n",
    "for smooth_type in ['grid','constant']:\n",
    "    df_sub\n",
    "    x=df_skill_smooth['smooth']\n",
    "\n",
    "plt.plot(df_skill['tau_d'],df_skill['r'],label='Pearson $r$, full')\n",
    "plt.plot(df_skill['tau_d'],df_skill['r_sub'],label='Pearson $r$, sub')\n",
    "\n",
    "plt.plot(df_skill['tau_d'],df_skill['rho'],label=r'Spearman $\\rho$, full')\n",
    "plt.plot(df_skill['tau_d'],df_skill['rho_sub'],label=r'Spearman $\\rho$, sub')\n",
    "\n",
    "plt.plot(df_skill['tau_d'],df_skill['rlog'],label='Pearson $r$, full, log transform')\n",
    "plt.plot(df_skill['tau_d'],df_skill['rlog_sub'],label='Pearson $r$, sub, log transform')\n",
    "plt.legend(loc='upper left',bbox_to_anchor=[1.02,1.0],frameon=False)\n",
    "plt.xlabel('Decay time scale (d)')\n",
    "plt.axis(ymin=0,xmin=0)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously I got these results:\n",
    "* Pearson $r$ is maximized by omitting any decay, as in $\\tau\\rightarrow\\infty$\n",
    "* Spearman $\\rho$ is maximized by having a very short decay, 15h\n",
    "* Log-transformed Pearson $r$ is maximized at around 25 days.\n",
    "\n",
    "If I remove the top 2 and bottom 2 observed values, then Pearson\n",
    "values come up to a level similar to log-pearson. Pearson\n",
    "is still maximized by having little or no decay.\n",
    "\n",
    "Re-running this, I don't see the local max in Pearson of the\n",
    "log-transformed data.\n",
    "\n",
    "What does it mean that Pearson correlation is highest at long decay\n",
    "time scales, while Spearman is highest at short decay scales?\n",
    "\n",
    "At short decay time scales, many of the predicted concentrations end up being\n",
    "very small, substantially smaller than the observations even with no scaling\n",
    "of the loads. This is penalized by Pearson, but for Spearman this correlates with\n",
    "the lower concentration in the ocean compared to the estuary, and thus increases\n",
    "skill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Spearman just for getting two populations split correctly?\n",
    "# Similar to what I get (for an offset that nearly makes the populations\n",
    "# distinct)\n",
    "# If the populations are different by a scale, then it takes a pretty \n",
    "# large scale (30?) to get a spearman of 0.7.\n",
    "\n",
    "N=60\n",
    "obs=np.random.random(N)\n",
    "mod=np.random.random(N)\n",
    "\n",
    "#off=0.8\n",
    "#obs[:N//2]+=off\n",
    "#mod[:N//2]+=off\n",
    "\n",
    "fac=30\n",
    "obs[:N//2]*=fac\n",
    "mod[:N//2]*=fac\n",
    "\n",
    "rho,p=spearmanr(obs,mod)\n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group samples - Central Bay, North Bay, South Bay, NMS.\n",
    "def loc_to_region(s):\n",
    "    if s.startswith('SPB') or s.startswith('SUB'): return 'NB'\n",
    "    if s.startswith('SB') or s.startswith('LSB'): return 'SB'\n",
    "    if s.startswith('GFNMS') or s.startswith('MBNMS') or s.startswith('CBNMS'): \n",
    "        return 'NMS'\n",
    "    if s.startswith('CB') or s=='SFBay': return 'CB'\n",
    "    raise Exception(\"No match for %s\"%s)\n",
    "    \n",
    "def analyze_medians(df):\n",
    "    dfm=df.loc[:,['region','obs','predicted']].melt(id_vars=['region']).set_index(['region','variable'])\n",
    "\n",
    "    dfm.sort_index(inplace=True)\n",
    "\n",
    "    def p_total(f):\n",
    "        # Maximize the combined median test\n",
    "        p_tot=1.0\n",
    "        for region in ['CB','NB','SB','NMS']:\n",
    "            obs=dfm.loc[ (region,'obs'), 'value']\n",
    "            mod=f*dfm.loc[ (region,'predicted'), 'value']\n",
    "            # is it better to choose just the modeled data where the observation\n",
    "            # is valid?\n",
    "            stat,p,M,cont=stats.median_test( obs[np.isfinite(obs)] ,\n",
    "                                             mod[np.isfinite(mod)] )\n",
    "            p_tot*=p\n",
    "        return p_tot\n",
    "\n",
    "    def cost(p):\n",
    "        return -p_total( 10**p )\n",
    "\n",
    "    from scipy.optimize import brute\n",
    "\n",
    "    best=brute(cost,[[-4,1]],Ns=51)\n",
    "    scale=10**best[0]\n",
    "    p_tot=p_total(scale)\n",
    "    print(f\"P-total: {p_tot:.3f}  with scale: {1/scale:5f}\")\n",
    "    return p_tot,scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pulling in the old code from median comparisons.\n",
    "\n",
    "manta_field='part_per_m2_nofiber'\n",
    "\n",
    "for tau_d in [50]: # [0.5,1,1.5,2.,3,4,5,6,8,10,12,15,20,25,30,35,40,45,50,55,60]:\n",
    "    print(f\"Tau: {tau_d} days\")\n",
    "    tau=np.timedelta64(int(tau_d*86400),'s')\n",
    "    df=predict_mantas(manta,tau,storm_factor=0.01)\n",
    "    \n",
    "    df=df[ df[manta_field].notnull() ].copy()\n",
    "    \n",
    "    regions=df['region']=df['SAMPLE LOCATION'].apply(loc_to_region)\n",
    "    df=df.rename(columns={manta_field:'obs','C_model':'predicted'}).loc[:,['region','obs','predicted']]\n",
    "\n",
    "    # Scale gets optimized to find the best nonparametric match\n",
    "    p_tot,scale=analyze_medians(df)\n",
    "\n",
    "    # This is not rigorous!\n",
    "    obs_mag=1.0\n",
    "    pred_mag=1/scale\n",
    "    \n",
    "    df['obs_frac']=df['obs'] / obs_mag\n",
    "    df['pred_frac']=df['predicted'] / pred_mag\n",
    "\n",
    "    dfm=df.loc[:,['region','obs_frac','pred_frac']].melt(id_vars=['region'])\n",
    "\n",
    "    fig,ax=plt.subplots(1,1)# ,figsize=(10,3))\n",
    "    sns.boxplot(x='region',y='value',hue='variable',data=dfm,fliersize=0,whis=0,\n",
    "                ax=ax,bootstrap=1000)\n",
    "    ax.axis(ymax=0.35,ymin=0)\n",
    "    ax.legend(ax.artists,['Observed',f'Predicted/{pred_mag:.2f}'],loc='upper right')\n",
    "    ax.set_ylabel('Particle m$^{-2}$')\n",
    "    ax.set_xlabel('Region')\n",
    "    \n",
    "    ax.set_title(f'Tau={tau_d} days  $\\\\Pi \\\\rho$={p_tot:.2f}')\n",
    "    df2=dfm.set_index(['region','variable'])\n",
    "    p_tot=1.0\n",
    "    for region in ['CB','NB','SB','NMS']:\n",
    "        stat,p,M,cont=stats.median_test( df2.loc[ (region,'obs_frac'), 'value'] ,\n",
    "                                         df2.loc[ (region,'pred_frac'), 'value'] )\n",
    "        p_tot*=p\n",
    "    print(f\"{tau_d} days: ampl. error: {pred_mag/obs_mag:.3f} Median test net p: {p_tot:.5f}\")\n",
    "    \n",
    "    print()\n",
    "    fig.savefig(f'manta_comp_v05-tau{tau_d:.1f}.png',dpi=200)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#plt.plot( df2.loc[('NB','obs_frac'),'value'], 'ro')\n",
    "#plt.plot( df2.loc[('NB','pred_frac'),'value'],'go')\n",
    "\n",
    "# Just 6 values...\n",
    "display(df2.loc[('NB','obs_frac'),'value'])\n",
    "display(df2.loc[('NB','pred_frac'),'value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each manta sample, make a plot showing the conc. distribution at that time\n",
    "# to debug what's up with the north bay samples:\n",
    "\n",
    "tau_d=8\n",
    "fac=39.81 # this is from analyze_medians.\n",
    "six.moves.reload_module(conc_figure)\n",
    "\n",
    "fig_dir=f\"sample_wise_tau{tau_d}\"\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "\n",
    "tau=np.timedelta64(int(tau_d*86400),'s')\n",
    "for idx,rec in manta.iterrows():\n",
    "    print(idx)\n",
    "    img_fn=os.path.join(fig_dir,f\"sample-{idx}.png\")\n",
    "    fig,axs=plt.subplots(1,2,figsize=(11,7))\n",
    "    fig.subplots_adjust(left=0,right=1,top=1,bottom=0,wspace=0.02)\n",
    "    df=particles_for_date(rec.DATE).copy()\n",
    "\n",
    "    age=df['time'] - df['rel_time']\n",
    "    decay=np.exp( -age/tau ) \n",
    "    \n",
    "    # No spatial weighting, but need the godin filter weights\n",
    "    t_sample=np.datetime64(rec.DATE)\n",
    "    # Noon, local, day of sampling\n",
    "    t_center = t_sample+np.timedelta64(8,'h') + np.timedelta64(12,'h')\n",
    "    delta_hours=((df['time']-t_center)/np.timedelta64(1,'h')).astype(np.int32)\n",
    "    weight_time=godwin[delta_hours+godwin_offset_h]\n",
    "\n",
    "    df['count']=weight_time * decay * df['mp_per_particle'] \n",
    "\n",
    "    weight=weight_time * decay * df['mp_per_particle'] \n",
    "        \n",
    "    ds_conc=post.particles_to_conc(df,grid,'count')\n",
    "\n",
    "    ds_smooth=ds_conc.copy()\n",
    "    for _ in range(30):\n",
    "        ds_smooth['conc']=('cell',), Msmooth.dot(ds_smooth.conc.values)\n",
    "    \n",
    "    cell=grid.select_cells_nearest([rec.x,rec.y])\n",
    "        \n",
    "    zoom=conc_figure.BayConcFigure.zoom\n",
    "    \n",
    "    if ( (rec.x>zoom[0]) and\n",
    "        (rec.x<zoom[1]) and\n",
    "        (rec.y>zoom[2]) and\n",
    "        (rec.y<zoom[3])):\n",
    "        FigCls=conc_figure.BayConcFigure\n",
    "        FigCls.cax_loc=[0.82,0.25,0.03,0.35] # normalized to axis \n",
    "    else:\n",
    "        FigCls=conc_figure.CoastalConcFigure\n",
    "        \n",
    "    for ds,ax in zip([ds_smooth,ds_conc],axs):\n",
    "        cf=FigCls(ds,fig=fig,ax=ax,grid=grid)\n",
    "        cf.ax.plot([rec.x],[rec.y],'k+')\n",
    "        conc_pred=ds.conc.isel(cell=cell).item()\n",
    "        conc_pred=conc_pred / fac\n",
    "\n",
    "        cf.ax.text(rec.x,rec.y,\n",
    "                   f\"part_per_m2_nofiber\\n obs={rec.part_per_m2_nofiber:.2e}\\npred={conc_pred:.2e}\")\n",
    "\n",
    "        cf.ax.text(0.01,0.95,f\"Manta sample: {rec.SampleID}\",transform=cf.ax.transAxes,va='top')\n",
    "    fig.savefig(img_fn,dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "-- \n",
    "\n",
    "2. Think through how to present the comparisons. *ish*\n",
    "3. What can we infer from these patterns? *ish*\n",
    "4. Results appear worse, in terms of likelihood, than \n",
    "   for the old runs, probably driven by NB. Looks like \n",
    "   I am underpredicting NB.  Sample by sample, not terribly\n",
    "   obvious what's up.  It's just 6 samples.\n",
    "4. Would having different loss coefficients for WW and SW be worth\n",
    "   attempting?\n",
    "    - This is worth trying, and try maximizing the pearson with outliers\n",
    "      tossed.\n",
    "    - Does not make much difference.\n",
    "5. Is it worth following a similar analysis to previously, lumping\n",
    "   samples into embayments? *Blindly running that same code, it suggests\n",
    "   that decay times >=20 days are best, with little difference beyond there.*\n",
    "   I'm getting smaller net prob. values, though, and it seems to be driven\n",
    "   by NB being underpredicted (relatively speaking) in the new runs. Any chance\n",
    "   I'm missing something in the north? The one map plot above looks okay, but compare\n",
    "   for wet weather.\n",
    "   \n",
    "What about applying a much more aggressive smoothing, going implicit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six.moves.reload_module(conc_figure)\n",
    "\n",
    "conc_figure.BayConcFigure(conc_ds,grid=grid,cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closer look at distributions\n",
    "---\n",
    "\n",
    "Tau constant at 25:\n",
    "\n",
    "The predictions with the best spearman were grid smoothing -10000.\n",
    "But the log-normal shape here is pretty terrible.\n",
    "\n",
    "A moderately better log-normal comes up with no smoothing.\n",
    "\n",
    "The best match in distribution is with no smoothing, tau=10,\n",
    "storm_factor 0.05 or 0.01.\n",
    "\n",
    "Given the amount of smoothing already taking place in time,\n",
    "and how much the results here are affected by smoothing in\n",
    "space, it may not be a great comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "fig,axs=plt.subplots(2,1)\n",
    "\n",
    "mp=manta_pred_master\n",
    "sel_tau=mp.tau==10\n",
    "sel_smooth=mp.Msmooth_label=='constant'\n",
    "sel_step=mp.step==0\n",
    "sel_storm=mp.storm_factor==0.01\n",
    "sel=sel_tau & sel_smooth & sel_step & sel_storm\n",
    "df=manta_pred_master[sel]\n",
    "\n",
    "nz=df['part_per_m2_nofiber']>0\n",
    "kde=False\n",
    "ret=sns.histplot(df['part_per_m2_nofiber'][nz],\n",
    "                 log_scale=True,kde=kde,bins=25,ax=axs[0])\n",
    "shape,loc,scale=stats.lognorm.fit(df['part_per_m2_nofiber'][nz],floc=0)\n",
    "x=10**np.linspace(-5,2,300)\n",
    "ln_pred=stats.lognorm.pdf(x,shape,loc=loc,scale=scale)\n",
    "axs[0].plot(x,x*ln_pred*nz.sum()/2,color='tab:orange',label='Log-normal')\n",
    "\n",
    "# ---- same, for model output\n",
    "pred=df['C_model'].values\n",
    "pred=pred[pred>0]\n",
    "\n",
    "ret=sns.histplot(pred,log_scale=True,kde=kde,bins=25,ax=axs[1])\n",
    "m_shape,m_loc,m_scale=stats.lognorm.fit(pred,floc=0)\n",
    "ln_pred=stats.lognorm.pdf(x,m_shape,loc=m_loc,scale=m_scale)\n",
    "axs[1].plot(x,x*ln_pred*nz.sum()/2,color='tab:orange',label='Log-normal')\n",
    "\n",
    "print(\"Obs scale: \",scale)\n",
    "print(\"Mod scale: \",m_scale)\n",
    "print(\"Obs shape: \",shape)\n",
    "print(\"Mod shape: \",m_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp=manta_pred_master\n",
    "sel_tau=mp.tau==60\n",
    "sel_smooth=mp.Msmooth_label=='grid'\n",
    "sel_step=mp.step==-100000\n",
    "sel_storm=mp.storm_factor==1.0\n",
    "sel=sel_tau & sel_smooth & sel_step & sel_storm\n",
    "df=manta_pred_master[sel]\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import spearmanr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodels on log-transform\n",
    "n_low=n_high=0\n",
    "obs_rank=np.argsort(np.argsort(df['part_per_m2_nofiber']))\n",
    "obs_sel=(obs_rank>=n_low) & (obs_rank<len(df)-n_high)\n",
    "mod_rank=np.argsort(np.argsort(df['C_model']))\n",
    "mod_sel=(mod_rank>=n_low) & (mod_rank<len(df)-n_high)\n",
    "\n",
    "sel_nz=(df['part_per_m2_nofiber']>0)&(df['C_model']>0).values\n",
    "sel=obs_sel&mod_sel&sel_nz\n",
    "print(f\"N={len(rank)}  Nsel={sel.sum()}\")\n",
    "\n",
    "data=df[sel].copy()\n",
    "data['log_mod']=np.log(data['C_model'])\n",
    "data['log_obs']=np.log(data['part_per_m2_nofiber'])\n",
    "\n",
    "res=smf.ols(formula='log_mod ~ log_obs', data=data).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho,rho_p=spearmanr(data['C_model'],data['part_per_m2_nofiber'])\n",
    "print(f\"Spearman:  rho={rho}  p-value={rho_p}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
